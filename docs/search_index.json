[["index.html", "SWAG Workshops Repository Chapter 1 Introduction", " SWAG Workshops Repository UW Statistical Workshops and Applications Group September 23, 2022 Chapter 1 Introduction Welcome to the Github repository for the Statistical Workshops and Applications Group (SWAG)! Each workshop will be given its own chapter in this document, where the content for the workshop will be shared. These workshops will be more interactive than presentations or lectures – and each workshop will typically be accompanied by relevant R code. For the Fall 2022 term, we have planned four workshops spanning several topics: sampling-resampling methods (Oct. 6), the Bernstein-von Mises theorem (Oct. 19), variational inference (Nov. 10), and smoothing techniques (Dec. 1). "],["chapter2.html", "Chapter 2 Sampling-Resampling Methods 2.1 Introduction 2.2 Rejection Sampling 2.3 Sampling-Resampling Methods 2.4 Implementation of Sampling-Resampling Methods 2.5 Sampling-Resampling in Multiple Dimensions", " Chapter 2 Sampling-Resampling Methods Luke Hagar October 6, 2022 2.1 Introduction Welcome to the first workshop for the Statistical Workshops and Applications Group (SWAG)! This workshop discusses sampling-resampling methods, mainly in the context of Bayesian inference. 2.2 Rejection Sampling Before introducing sampling-resampling methods, we briefly discuss rejection sampling (Ripley 2009) along with its strengths and shortcomings. In what follows, we discuss the set up for our sampling scenario. Let’s suppose we can easily generate a sample from a continuous density function \\(g(\\boldsymbol{\\theta})\\). We call this the proposal distribution. However, we want a sample from a density \\(h(\\boldsymbol{\\theta})\\) such that \\(g(\\boldsymbol{\\theta}) = 0\\) implies \\(h(\\boldsymbol{\\theta}) = 0\\) for all \\(\\boldsymbol{\\theta} \\in \\boldsymbol{\\Theta}\\). We have a positive function \\(f(\\boldsymbol{\\theta})\\) that is proportional to \\(h(\\boldsymbol{\\theta})\\). That is, \\(h(\\boldsymbol{\\theta}) = f(\\boldsymbol{\\theta})/\\int f(\\boldsymbol{\\theta})d\\boldsymbol{\\theta}\\). For rejection sampling, we further suppose that there exists an identifiable constant \\(M &gt; 0\\) such that \\(f(\\boldsymbol{\\theta})/g(\\boldsymbol{\\theta}) \\le M\\) for all \\(\\boldsymbol{\\theta} \\in \\boldsymbol{\\Theta}\\). Let’s reinforce this terminology with an example. In this example, we want to sample from \\(h(\\theta) = 12\\theta^2(1-\\theta)\\) for \\(0 &lt; \\theta &lt; 1\\). This is actually a \\(\\text{BETA}(3,2)\\) distribution. For this example, we let \\(f(\\theta) = h(\\theta)\\). We have that \\(f(\\theta) \\le 16/9\\) for all \\(0 &lt; \\theta &lt; 1\\). We can easily sample over the unit interval using the uniform density \\(U(0,1)\\). This corresponds to choosing \\(g(\\theta) = 1\\) for \\(0 &lt; \\theta &lt; 1\\). It follows that \\(f(\\boldsymbol{\\theta})/g(\\boldsymbol{\\theta}) \\le 16/9 = M\\) for all \\(0 &lt; \\theta &lt; 1\\). This scenario is visualized in Figure 2.1. ## compute f and g across the range of theta for plotting purposes theta &lt;- seq(0,1,by = 0.005) f &lt;- 12*theta^2*(1-theta) g &lt;- rep(1, length(theta)) M &lt;- 16/9 ## generate the plot and corresponding legend plot(theta, f, col = &quot;steelblue&quot;, type = &quot;l&quot;, ylab = &quot;Density&quot;, ylim = c(0,2.25), xlab = expression(theta)) lines(theta, M*g, lty = 2) legend(&quot;topright&quot;, c(&quot;f&quot;, &quot;M*g&quot;), col = c(&quot;steelblue&quot;, &quot;black&quot;), lty = c(1,2), bty = &quot;n&quot;) Figure 2.1: Visualization of rejection sampling functions We now introduce rejection sampling. This process is detailed in Algorithm 1. A sample of \\(\\boldsymbol{\\theta}\\) values accepted via Algorithm 1 is a sample from the density of interest \\(h(\\boldsymbol{\\theta})\\) (Ripley 2009). Algorithm 1 Generate \\(\\boldsymbol{\\theta}\\) from \\(g(\\boldsymbol{\\theta})\\). Generate \\(u \\sim U(0,1)\\). If \\(u \\le f(\\boldsymbol{\\theta})/(Mg(\\boldsymbol{\\theta}))\\), return \\(\\boldsymbol{\\theta}\\); otherwise return to Step 1. We now illustrate how to implement rejection sampling to generate 10000 observations from \\(h(\\boldsymbol{\\theta})\\) for this simple example. The results are visualized in Figure 2.2. ## initialize matrices for accepted and rejected theta values accept_mat &lt;- NULL; reject_mat &lt;- NULL ## set seed for reproducibility set.seed(1) stop &lt;- FALSE ## continue until 10000 points are accepted while (stop == FALSE){ ## generate a value from g: U(0,1) theta_temp &lt;- runif(1) ## generate u to decide whether to accept/reject u &lt;- runif(1) ## compute the values for f and g at this theta value f_temp &lt;- 12*theta_temp^2*(1-theta_temp) g_temp &lt;- 1 ## decide whether or not to accept point; ## we save the rejected points and &quot;u&quot; realizations for the plot if (u &lt;= f_temp/(M*g_temp)){ accept_mat &lt;- rbind(accept_mat, c(theta_temp,u)) if (nrow(accept_mat) &gt;= 10000){ stop &lt;- TRUE } } else{ reject_mat &lt;- rbind(reject_mat, c(theta_temp,u)) } } ## generate the plot and corresponding legend ## green points represent accepted (theta, u) combinations ## red points represent rejected (theta, u) combinations plot(theta, f, col = &quot;steelblue&quot;, type = &quot;l&quot;, ylab = &quot;M*u&quot;, ylim = c(0,2.25), xlab = expression(theta), lwd = 1.5) lines(theta, M*g, lty = 2, lwd = 1.5) legend(&quot;topright&quot;, c(&quot;f&quot;, &quot;M*g&quot;), col = c(&quot;steelblue&quot;, &quot;black&quot;), lty = c(1,2), bty = &quot;n&quot;) points(accept_mat[,1], M*accept_mat[,2], col = adjustcolor(&quot;seagreen&quot;,0.05), pch = 19) points(reject_mat[,1], M*reject_mat[,2], col = adjustcolor(&quot;firebrick&quot;,0.05), pch = 19) Figure 2.2: Visualization of rejection sampling with accepted points (green) and rejected points (red) The points in Figure 2.2 depict which \\(\\theta\\) values were accepted (green) and rejected (red) by rejection sampling. The resulting sample appears to come from the \\(\\text{BETA}(3,2)\\) distribution. We make two final remarks about rejection sampling that bear relevance to sampling-resampling methods. To implement rejection sampling, we must identify a constant \\(M &gt; 0\\) such that \\(f(\\boldsymbol{\\theta})/g(\\boldsymbol{\\theta}) \\le M\\) for all \\(\\boldsymbol{\\theta} \\in \\boldsymbol{\\Theta}\\). This choice for \\(M\\) is important. For this example, we could have chosen \\(M &gt; 16/9\\), but the sampling procedure would have been less efficient. Choosing \\(M &lt; 16/9\\) would have returned a sample that is not from the \\(\\text{BETA}(3,2)\\) distribution. We need to carefully consider the support of \\(h(\\boldsymbol{\\theta})\\) and \\(g(\\boldsymbol{\\theta})\\). Choosing \\(g(\\boldsymbol{\\theta})\\) to be \\(U(0.25, 0.75)\\) would not have been appropriate for this example. Choosing \\(g(\\boldsymbol{\\theta})\\) to be \\(U(-1, 1)\\) would be fine but less efficient. When using sampling-resampling methods, remark 1 is often not of concern. 2.3 Sampling-Resampling Methods 2.3.1 Overview of the Sampling-Importance-Resampling Algorithm We now discuss how to obtain an approximate sample from the density \\(h(\\boldsymbol{\\theta})\\) using a particular weighted sampling-resampling method, which is a variant of the bootstrap procedure (Efron 1982). This method was first formally proposed in Rubin (1987) and Rubin (1988) as the sampling-importance-resampling (SIR) algorithm. We introduce this procedure via Algorithm 2, which uses notation from Smith and Gelfand (1992). Algorithm 2 Generate \\(\\boldsymbol{\\theta}_i, ~ i = 1,...,n\\) from \\(g(\\boldsymbol{\\theta})\\). For each \\(\\boldsymbol{\\theta}_i\\), compute \\(\\omega_i = f(\\boldsymbol{\\theta}_i)/g(\\boldsymbol{\\theta_i})\\). Let \\(q_i = \\omega_i/\\sum_{j=1}^n\\omega_j\\) for \\(i = 1,...,n\\). Draw \\(\\{\\boldsymbol{\\theta}^*_1,..., \\boldsymbol{\\theta}^*_m\\}\\) from the discrete distribution over \\(\\{\\boldsymbol{\\theta}_1, ..., \\boldsymbol{\\theta}_n \\}\\) with replacement, where mass \\(q_i\\) is placed on \\(\\boldsymbol{\\theta}_i\\). Under several conditions, \\(\\boldsymbol{\\theta}^*\\) is approximately distributed according to \\(h(\\boldsymbol{\\theta})\\). Algorithm 2 returns approximate samples from \\(h(\\boldsymbol{\\theta})\\) when \\(n\\) is sufficiently large and \\(supp(h) \\subseteq supp(g)\\), where \\(supp()\\) refers to the support of a probability distribution. How large \\(n\\) must be depends on the extent to which \\(h(\\boldsymbol{\\theta})\\) resembles \\(g(\\boldsymbol{\\theta})\\). Moreover, \\(m\\) is typically smaller than \\(n\\); Rubin (1987) stated that specifying \\(m\\) and \\(n\\) such that their ratio \\(n/m\\) is at least 20 should be sufficient in most scenarios. When implementing this sampling-resampling procedure, we must still be mindful of the supports of \\(h(\\boldsymbol{\\theta})\\) and \\(g(\\boldsymbol{\\theta})\\). However, we do not need to identify a constant \\(M &gt; 0\\) such that \\(f(\\boldsymbol{\\theta})/g(\\boldsymbol{\\theta}) \\le M\\) for all \\(\\boldsymbol{\\theta} \\in \\boldsymbol{\\Theta}\\). We only require a function \\(g(\\boldsymbol{\\theta})\\) that we can readily sample from and a function \\(f(\\boldsymbol{\\theta})\\) that is proportional to our density of interest \\(h(\\boldsymbol{\\theta})\\). For Bayesian inference, we often want to sample from a posterior distribution of interest \\(\\pi(\\boldsymbol{\\theta}| \\boldsymbol{x})\\) for \\(\\boldsymbol{\\theta}\\) given observed data \\(\\boldsymbol{x}\\). Let \\(L()\\) be the likelihood function and \\(p()\\) be the prior for \\(\\boldsymbol{\\theta}\\). The posterior distribution communicates which values \\(\\boldsymbol{\\theta}\\) are plausible given the observed data and our prior beliefs. By Bayes’ Theorem, we have that \\[\\pi(\\boldsymbol{\\theta}| \\boldsymbol{x}) = \\frac{L(\\boldsymbol{\\theta}; \\boldsymbol{x})p(\\boldsymbol{\\theta})}{\\int L(\\boldsymbol{\\theta}; \\boldsymbol{x})p(\\boldsymbol{\\theta})d\\boldsymbol{\\theta}} \\propto L(\\boldsymbol{\\theta}; \\boldsymbol{x})p(\\boldsymbol{\\theta}).\\] Thus, we have \\(h(\\boldsymbol{\\theta}) = \\pi(\\boldsymbol{\\theta}| \\boldsymbol{x})\\) and \\(f(\\boldsymbol{\\theta}) = L(\\boldsymbol{\\theta}; \\boldsymbol{x})p(\\boldsymbol{\\theta})\\) using the notation from earlier. For a given \\(\\boldsymbol{\\theta}\\) value, it is straightforward to compute \\(f(\\boldsymbol{\\theta})\\); however, we are often not able to compute \\(h(\\boldsymbol{\\theta})\\) directly. As such, the SIR algorithm goes well with the framework for Bayesian inference. It can be used to generate an approximate sample from the posterior distribution of interest \\(h(\\boldsymbol{\\theta}) = \\pi(\\boldsymbol{\\theta}| \\boldsymbol{x})\\) and serves as an alternative to more standard Markov chain Monte Carlo (MCMC) methods in many scenarios. The SIR algorithm can be used with non-Bayesian methods, but the remainder of this workshop focuses on implementing the SIR algorithm to conduct approximate posterior sampling. 2.3.2 Computational Considerations The SIR algorithm often produces more stable results when \\(\\omega_i, i = 1,...,n\\) are computed on the logarithmic scale. This is consistent with other methods for Bayesian inference that do not involve sampling-resampling. Particularly when \\(\\boldsymbol{\\theta}\\) is multivariate, \\(f(\\boldsymbol{\\theta})\\) and \\(g(\\boldsymbol{\\theta})\\) can take small positive values that are extremely close to 0. Computing \\(\\text{log}(\\omega_i)\\) helps make underflow errors less common, as we will see shortly. We have that \\[\\text{log}(\\omega_i) = \\text{log}(f(\\boldsymbol{\\theta}_i)) - \\text{log}(g(\\boldsymbol{\\theta}_i)).\\] In the context of Bayesian inference, \\(f(\\boldsymbol{\\theta}) = L(\\boldsymbol{\\theta}; \\boldsymbol{x})p(\\boldsymbol{\\theta})\\) is generally the product of several terms, so \\(\\text{log}(f(\\boldsymbol{\\theta}_i))\\) is often easier to manipulate than \\(f(\\boldsymbol{\\theta}_i)\\). When \\(g(\\boldsymbol{\\theta})\\) is a well known probability distribution, we can often compute \\(\\text{log}(g(\\boldsymbol{\\theta}_i))\\) using built-in R functions. For instance, if \\(g(\\boldsymbol{\\theta})\\) is the standard normal distribution, we can compute \\(\\text{log}(g(\\boldsymbol{\\theta}_i))\\) as \\(\\texttt{dnorm(}\\boldsymbol{\\theta}_i\\texttt{, log = TRUE)}\\). We must then exponentiate the \\(\\text{log}(\\omega_i)\\) values to compute the normalized weights \\(q_i, ~ i = 1, ..., n\\). In certain situations, we proceed with caution to avoid more underflow errors. Let’s consider the following example with only three \\(\\text{log}(\\omega_i)\\) values: \\(\\text{log}(\\omega_1) = 0\\), \\(\\text{log}(\\omega_2) = 1\\), and \\(\\text{log}(\\omega_3) = 2\\). For this example, we can exponentiate and standardize these weights without issues using the following R code. ## define log weights log_omega &lt;- c(0, 1, 2) ## exponentiate the weights omega &lt;- exp(log_omega) ## standardize the weights and print results by enclosing assignment command with () (q &lt;- omega/sum(omega)) ## [1] 0.09003057 0.24472847 0.66524096 Here, we standardize the weights after exponentiation. That being the case, we can apply a translation to the weights on the logarithmic scale without impacting the final standardized weights. For this example, the maximum \\(\\text{log}(\\omega_i)\\) is \\(\\text{log}(\\omega_3) = 2\\). Let’s subtract 2 from each original \\(\\text{log}(\\omega_i)\\) value before exponentiation and output the results. ## define log weights log_omega &lt;- c(0, 1, 2) ## subtract 2 from each log_omega value and exponentiate the weights omega &lt;- exp(log_omega - max(log_omega)) ## standardize the weights (q &lt;- omega/sum(omega)) ## [1] 0.09003057 0.24472847 0.66524096 The final standardized weights are the same! This trick is useful when the \\(\\text{log}(\\omega_i)\\) values are very small (or very large). Let’s now imagine that \\(\\text{log}(\\omega_1) = -800\\), \\(\\text{log}(\\omega_2) = -799\\), and \\(\\text{log}(\\omega_3) = -798\\). Let’s see what occurs if we try to exponentiate these values. ## define log weights log_omega &lt;- c(-800, -799, -798) ## exponentiate the weights (omega &lt;- exp(log_omega)) ## [1] 0 0 0 ## standardize the weights (q &lt;- omega/sum(omega)) ## [1] NaN NaN NaN This produces an underflow error. The values for \\(\\omega_i, ~ i = 1, 2, 3\\) are so small that they are rounded down to 0. But, we can apply a translation to the \\(\\text{log}(\\omega_i)\\) values by subtracting \\(\\text{max}_{i = 1, 2, 3}~\\text{log}(\\omega_i) = -798\\) from each value. This produces the same final standardized weights as for the earlier example – which makes sense because this example simply subtracted 800 from each of the earlier \\(\\text{log}(\\omega_i)\\) values. ## define log weights log_omega &lt;- c(-800, -799, -798) ## subtract max(log_omega) from each log_omega value and exponentiate the weights omega &lt;- exp(log_omega - max(log_omega)) ## standardize the weights (q &lt;- omega/sum(omega)) ## [1] 0.09003057 0.24472847 0.66524096 2.4 Implementation of Sampling-Resampling Methods 2.4.1 Illustrative Example with Binary Data Here, we demonstrate how to implement the SIR algorithm to faciliate posterior sampling for a simple example. In this example, we assume that we observe binary data \\(x_1, ..., x_N\\) such that \\(x_j \\sim \\text{BIN}(1, \\theta)\\). We assume that the prior for \\(\\theta\\) is an uninformative \\(\\text{BETA}(1,1)\\) prior, which implies that \\(p(\\theta) = 1\\) for \\(0 &lt; \\theta &lt; 1\\). In this case, the posterior \\(h(\\theta) = \\pi(\\theta| \\boldsymbol{x})\\) is such that \\[h(\\boldsymbol{\\theta}) \\propto L(\\theta; \\boldsymbol{x}) = \\prod_{j=1}^N\\theta^{x_j}(1 - \\theta)^{1-x_j} = \\theta^{\\sum_{j=1}^Nx_j}(1-\\theta)^{N-\\sum_{j=1}^Nx_j} = f(\\theta).\\] Let’s assume that we have \\(N = 20\\) observations and that 15 of these Bernoulli trials were successful (\\(\\sum_{j=1}^N x_j = 15\\)). It follows that \\(f(\\theta) = \\theta^{15}(1-\\theta)^5\\) for \\(0 &lt; \\theta &lt; 1\\). For this example, \\(h(\\theta)\\) is actually known: it is a \\(\\text{BETA}(16, 6)\\) distribution. Therefore, we do not need to use sampling-resampling methods to sample from \\(h(\\boldsymbol{\\theta})\\). However, because \\(h(\\theta)\\) is known, we can use this example to explore the performance of the SIR algorithm for different proposal distributions \\(g(\\boldsymbol{\\theta})\\). The first proposal distribution that we consider is \\(g(\\theta) = 1\\) for \\(0 &lt; \\theta &lt; 1\\) (i.e., the \\(U(0,1)\\) distribution). We plot \\(h(\\theta)\\) and \\(g(\\theta)\\) in Figure 2.3. ## compute h and g across the range of theta for plotting purposes theta &lt;- seq(0,1,by = 0.005) h &lt;- dbeta(theta, 16, 6) g &lt;- rep(1, length(theta)) ## generate the plot and corresponding legend plot(theta, h, col = &quot;steelblue&quot;, type = &quot;l&quot;, ylab = &quot;Density&quot;, ylim = c(0,4.5), xlab = expression(theta)) lines(theta, g, lty = 2) legend(&quot;topright&quot;, c(&quot;h&quot;, &quot;g&quot;), col = c(&quot;steelblue&quot;, &quot;black&quot;), lty = c(1,2), bty = &quot;n&quot;) Figure 2.3: Visualization of posterior and uniform sampling function While \\(h(\\theta)\\) and \\(g(\\theta)\\) have the same support, it does not appear that \\(U(0,1)\\) will be a very efficient proposal distribution for this example. Values of \\(\\theta_i\\) close to 0 that are generated via \\(g(\\theta)\\) will be assigned standardized weights of \\(q_i \\approx 0\\). We now show how to implement the entire SIR algorithm for this example with the uniform sampling function. We use the settings \\(n\\) = 1 million and \\(m = 50000\\). The results are visualized in Figure 2.4. ## input the sample of 20 observations into R x &lt;- c(rep(1,15), rep(0,5)) ## extract the number of observations (N) and successes N &lt;- length(x); sum_x &lt;- sum(x) ## define SIR algorithm settings n &lt;- 1000000; m &lt;- 50000 ## define a function that is proportional to the posterior (on the logarithmic scale) propLog &lt;- function(theta, obs, sum_data){ ## return -Inf is theta is not between 0 and 1 (this will exponentiate to give q_i = 0) return(ifelse(theta &gt; 0 &amp; theta &lt; 1, sum_data*log(theta) + (obs - sum_data)*log(1-theta), -Inf)) } set.seed(2) ## sample from proposal distribution samp &lt;- runif(n) ## form importance sampling weights on log-scale; ## here, log(g(theta)) = log(1) = 0 for all 0 &lt; theta &lt; 1 w1 &lt;- propLog(samp, N, sum_x) - 0 w1 &lt;- exp(w1 - max(w1)) q1 &lt;- w1/sum(w1) ## resample to create approximate sample from posterior inds1 &lt;- sample(seq(1,n,by = 1), size = m, replace = TRUE, prob = q1) post1 &lt;- samp[inds1] ## generate the plot and corresponding legend hist(post1, col = adjustcolor(&quot;grey50&quot;, 0.15), main = &quot;&quot;, freq = FALSE, xlim = c(0.2,1), xlab = expression(theta), ylab = &quot;Density&quot;, ylim = c(0, 4.5)) lines(theta, h, col = &quot;steelblue&quot;, type = &quot;l&quot;) lines(theta, g, lty = 2) legend(&quot;topright&quot;, c(&quot;h&quot;, &quot;g&quot;), col = c(&quot;steelblue&quot;, &quot;black&quot;), lty = c(1,2), bty = &quot;n&quot;) Figure 2.4: Histogram of posterior sample 1 obtained using SIR (with densities for \\(h\\) and \\(g\\)) The uniform proposal distribution appears to be serviceable for this example. The density curve for \\(h(\\theta)\\) – which is not known exactly in most situations – agrees well with histogram of the approximate posterior sample. However, the simulation results confirm that this distribution is not very efficient. The histogram of the standardized weights for this simulation is given in Figure 2.5. ## generate the plot and corresponding legend hist(q1, col = adjustcolor(&quot;grey50&quot;, 0.15), main = &quot;&quot;, xlab = expression(q), ylab = &quot;Frequency&quot;) Figure 2.5: Histogram of standardized weights from SIR algorithm (uniform proposal) If we were to use \\(h(\\theta)\\) as the proposal, all weights should be equal to \\(n^{-1} = 1000000^{-1} = 1 \\times 10^{-6}\\). In Figure 2.5, we can see that many of the standarized weights are extremely close to 0, and these \\(\\theta\\) values are very unlikely to be selected during the weighted sampling process. In contrast, some of the weights are more than 4 times greater than \\(1 \\times 10^{-6}\\). If we do not generate many points from \\(g(\\theta)\\) (\\(n\\) is small), then our posterior sample may not have many unique points. This issue can be overcome by generating a very large sample of points from the proposal distribution. We now discuss other potential choices for the proposal distribution for this example. 2.4.2 Practical Considerations for Choosing Proposal Distributions Diffuse distributions that are easy to sample from are often popular choices for proposal distributions. In particular, the uniform and normal distributions are often parameterized appropriately to serve this purpose. There are adaptive extensions to the SIR algorithm, including those by West (1993) and Givens and Raftery (1996). These methods typically specify a diffuse proposal distribution to obtain an initial sample from \\(h(\\boldsymbol{\\theta})\\). This initial sample from \\(h(\\boldsymbol{\\theta})\\) then informs a better proposal distribution and the SIR algorithm is run again. This process is repeated as necessary. We do not discuss these methods further in this workshop; we focus on the case where an approximate sample is obtained by running the SIR algorithm one time with a given proposal distribution. We now show how to implement the SIR algorithm for this example with the normal proposal density \\(N(0.75, 0.15)\\), where \\(0.15\\) is the standard deviation. We discuss why this is an appropriate proposal distribution shortly. The results are visualized in Figure 2.6. set.seed(3) ## define parameters for proposal distribution mu_g &lt;- 0.75 sigma_g &lt;- 0.15 ## sample from proposal distribution samp &lt;- rnorm(n, mu_g, sigma_g) ## form importance sampling weights on log-scale; w2 &lt;- propLog(samp, N, sum_x) - dnorm(samp, mu_g, sigma_g, log = TRUE) w2 &lt;- exp(w2 - max(w2)) q2 &lt;- w2/sum(w2) ## resample to create approximate sample from posterior inds2 &lt;- sample(seq(1,n,by = 1), size = m, replace = TRUE, prob = q2) post2 &lt;- samp[inds2] ## generate the plot and corresponding legend hist(post2, col = adjustcolor(&quot;grey50&quot;, 0.15), main = &quot;&quot;, freq = FALSE, xlim = c(0.2,1), xlab = expression(theta), ylab = &quot;Density&quot;, ylim = c(0, 4.5)) lines(theta, h, col = &quot;steelblue&quot;, type = &quot;l&quot;) lines(theta, dnorm(theta, mu_g, sigma_g), lty = 2) legend(&quot;topright&quot;, c(&quot;h&quot;, &quot;g&quot;), col = c(&quot;steelblue&quot;, &quot;black&quot;), lty = c(1,2), bty = &quot;n&quot;) Figure 2.6: Histogram of posterior sample 2 obtained using SIR (with densities for \\(h\\) and \\(g\\)) We can see the \\(N(0.75, 0.15)\\) proposal distribution performs well for this example. The density curve for \\(h(\\theta)\\) coincides with histogram of the approximate posterior sample. We can see that the density curve for this proposal distribution \\(g(\\theta)\\) resembles the posterior of interest \\(h(\\theta)\\) more than the uniform proposal distribution. In fact, the modes of \\(h(\\theta)\\) and \\(g(\\theta)\\) are the same: 0.75. We note that the support of the normal distribution is not restricted to the unit interval. However, this is not a problem. The \\(\\theta_i\\) values generated via \\(g(\\theta)\\) such that \\(\\theta_i \\notin (0,1)\\) are assigned standardized sampling weights of \\(q_i = 0\\). The histogram of the standardized weights for this simulation, given in Figure 2.7, suggests that this proposal distribution is more efficient than the previous one: the standardized weights take values closer to \\(1 \\times 10^{-6}\\) than for the previous example. We discuss numerical methods to compare proposal distributions in the next subsection. ## generate the plot and corresponding legend hist(q2, col = adjustcolor(&quot;grey50&quot;, 0.15), main = &quot;&quot;, xlab = expression(q), ylab = &quot;Frequency&quot;) Figure 2.7: Histogram of standardized weights from SIR algorithm (normal proposal 1) We now explore what occurs when we implement the SIR algorithm for this example with an inappropriate proposal distribution. We choose the normal proposal density \\(N(0.85, 0.05)\\), where \\(0.05\\) is the standard deviation. We discuss why this is not an appropriate proposal distribution shortly. The results are visualized in Figure 2.8. set.seed(5) ## define parameters for proposal distribution mu_g &lt;- 0.85 sigma_g &lt;- 0.05 ## sample from proposal distribution samp &lt;- rnorm(n, mu_g, sigma_g) ## form importance sampling weights on log-scale; w3 &lt;- propLog(samp, N, sum_x) - dnorm(samp, mu_g, sigma_g, log = TRUE) w3 &lt;- exp(w3 - max(w3)) q3 &lt;- w3/sum(w3) ## resample to create approximate sample from posterior inds3 &lt;- sample(seq(1,n,by = 1), size = m, replace = TRUE, prob = q3) post3 &lt;- samp[inds3] ## generate the plot and corresponding legend hist(post3, col = adjustcolor(&quot;grey50&quot;, 0.15), main = &quot;&quot;, freq = FALSE, xlim = c(0.2,1), xlab = expression(theta), ylab = &quot;Density&quot;, ylim = c(0, 8)) lines(theta, h, col = &quot;steelblue&quot;, type = &quot;l&quot;) lines(theta, dnorm(theta, mu_g, sigma_g), lty = 2) legend(&quot;topright&quot;, c(&quot;h&quot;, &quot;g&quot;), col = c(&quot;steelblue&quot;, &quot;black&quot;), lty = c(1,2), bty = &quot;n&quot;) Figure 2.8: Histogram of posterior sample 3 obtained using SIR (with densities for \\(h\\) and \\(g\\)) We can see the \\(N(0.85, 0.05)\\) proposal distribution does not perform well for this example. The density curve for \\(h(\\theta)\\) deviates histogram of the approximate posterior sample. Although the support for \\(g(\\theta)\\) is \\(\\theta \\in \\mathbb{R}\\), \\(g(\\theta)\\) is practically 0 for \\(\\theta &lt; 0.6\\). It is extremely unlikely that we will sample \\(\\theta\\) values less than 0.6 for finite \\(n\\), and they will not appear in the final sample. In practice, we do not know \\(h(\\theta)\\), so we cannot compare the distribution of the sample to the density of \\(h(\\theta)\\). However, we can compare the distribution of the approximate posterior sample to the proposal \\(g(\\theta)\\). We can see that many of the \\(\\theta\\) values in our sample correspond to extreme values of the proposal distribution \\(g(\\theta)\\); this suggests that the proposal distribution may not be diffuse enough. If \\(\\boldsymbol{\\theta}\\) is multivariate, then we could compare low dimensional projections of the sample distribution and \\(g(\\boldsymbol{\\theta})\\). ## return maximum standardized weight (max(q3)) ## [1] 0.01455077 The maximum standardized weight for this simulation is 0.01455. This is 14550 times larger than \\(1 \\times 10^{-6}\\), which indicates that the proposal distribution does not resemble the density of interest \\(h(\\theta)\\) very much. 2.4.3 Comparing Proposal Distributions Here, we consider two numerical metrics to evaluate the suitability of the proposal distribution that are presented in Givens and Raftery (1996). If the typically unknown \\(h(\\boldsymbol{\\theta})\\) were used as a proposal distribution, all standardized sampling weights would take a value of \\(1 \\times 10^{-6}\\). When most sampling weights take values close to \\(1 \\times 10^{-6}\\), our final approximate sample of size \\(m\\) should contain many unique points. That is, our sample should not be comprised of several points that are each selected many times. We let \\(Q\\) be the number of unique points in our final sample of \\(m\\) values. When all sampling weights are constant (i.e., equal to \\(1/n\\)), then \\(\\mathbb{E}(Q) \\approx n(1- \\text{exp}(-m/n))\\). We then define \\[U = \\frac{Q}{n(1- \\text{exp}(-m/n))}.\\] \\(U\\) generally takes values between 0 and 1. We note that \\(U\\) can take values of greater than 1, particularly when the proposal density closely resembles \\(h(\\boldsymbol{\\theta})\\). However, this is not likely to be the case when \\(g(\\boldsymbol{\\theta})\\) and \\(h(\\boldsymbol{\\theta})\\) differ substantially. Generally, larger values of \\(U\\) are preferred. We now compute the \\(U\\) values for each of the three proposal densities considered for this illustrative example: \\(U(0,1)\\), \\(N(0.75, 0.15)\\), and \\(N(0.85, 0.05)\\). ## compute denominator for U (number of m = 50000 points expected to be unique under proposal h) (denom &lt;- n*(1- exp(-m/n))) ## [1] 48770.58 ## compute U for U(0,1) (u1 &lt;- round(length(unique(inds1))/denom,4)) ## [1] 0.9511 ## compute U for N(0.75, 0.15) (u2 &lt;- round(length(unique(inds2))/denom,4)) ## [1] 0.9943 ## compute U for N(0.85, 0.05) (u3 &lt;- round(length(unique(inds3))/denom,4)) ## [1] 0.6875 The \\(U\\) values are 0.9511 for \\(U(0,1)\\), 0.9943 for \\(N(0.75, 0.15)\\), and 0.6875 for \\(N(0.85, 0.05)\\). The \\(N(0.75, 0.15)\\) proposal distribution has the largest \\(U\\) value, which agrees with informal insights drawn from the histograms of the standardized sampling weights. Similarly, we could also consider the mean squared distance between \\(n\\) times the standardized sampling weights and 1. To this end, we let \\[D = n^{-1}\\sum_{i=1}^n(n\\times q_i - 1)^2 = n\\sum_{i=1}^n(q_i - n^{-1})^2,\\] where \\(n\\) = 1 million for this example. This metric \\(D\\) penalizes proposal densities that give rise to situations where a few small regions of its support yield extremely large standardized sampling weights. Generally, smaller values of \\(D\\) are preferred. We now compute the \\(D\\) values for each of the three proposal densities considered for this illustrative example. ## compute D for U(0,1) (d1 &lt;- round(n*sum((q1 - 1/n)^2),3)) ## [1] 2.046 ## compute D for N(0.75, 0.15) (d2 &lt;- round(n*sum((q2 - 1/n)^2),3)) ## [1] 0.291 ## compute D for N(0.85, 0.05) (d3 &lt;- round(n*sum((q3 - 1/n)^2),3)) ## [1] 658.621 The \\(D\\) values are 2.046 for \\(U(0,1)\\), 0.291 for \\(N(0.75, 0.15)\\), and 658.621 for \\(N(0.85, 0.05)\\). The \\(N(0.75, 0.15)\\) proposal distribution has the smallest \\(D\\) value. Givens and Raftery (1996) suggested comparing competing proposal distributions using the ratio of their D values. In this case, the \\(N(0.75, 0.15)\\) proposal is about 7 times better than the \\(U(0,1)\\) proposal. 2.5 Sampling-Resampling in Multiple Dimensions 2.5.1 Exercise with Illustrative Example We now briefly consider how to implement the SIR algorithm for a multivariate example. In this example, we assume that we observe data \\(x_1, x_2 &gt;0\\). Here, we assume that \\(\\boldsymbol{\\theta} = (\\alpha, \\beta)^T\\) for \\(0 &lt; \\alpha &lt; 1\\) and \\(0 &lt; \\beta &lt; 1\\). We independently assume a \\(U(0,1)\\) prior for \\(\\alpha\\) and \\(\\beta\\), which implies that \\(p(\\alpha, \\beta) = 1\\) for \\(0 &lt; \\alpha, \\beta &lt; 1\\). In this case, the posterior \\(h(\\alpha, \\beta) = \\pi(\\alpha, \\beta| \\boldsymbol{x})\\) is proportional to the likelihood function that we define below: \\[h(\\alpha, \\beta) \\propto L(\\alpha, \\beta; \\boldsymbol{x}) = \\alpha(1-\\alpha)\\beta(1-\\beta)\\text{exp}(-x_1\\alpha -\\beta^{x_2}) = f(\\alpha,\\beta).\\] Let’s assume that we observe \\(x_1 = 3\\) and \\(x_2 = 5\\) We will use the prior \\(p(\\alpha, \\beta) = 1\\) for \\(0 &lt; \\alpha, \\beta &lt; 1\\) as the proposal density \\(g(\\alpha, \\beta)\\). You can complete the following code block to generate a contour plot of the posterior sample, depicted in Figure 2.9. A helper function is provided to compute \\(\\text{log}(f(\\alpha, \\beta))\\). In general, it can be more difficult to chose proposal densities for multivariate posteriors of interest, especially when the support is unbounded. Adaptive sampling-resampling methods can be quite useful in these situations. Moreover, we generally require larger values for \\(n\\) and \\(m\\) used with the SIR algorithm when the domain over which we want to sample is multivariate. ## define SIR algorithm settings n &lt;- 2500000; m &lt;- 125000 ## define a function that is proportional to the posterior (on the logarithmic scale) propLog &lt;- function(alpha, beta){ return(log(alpha) + log(1-alpha) + log(beta) + log(1-beta) - 3*alpha - beta^5) } set.seed(6) ## sample from proposal distribution (for alpha and beta) samp_alpha &lt;- runif(n) samp_beta &lt;- runif(n) ## form importance sampling weights on log-scale w &lt;- propLog(samp_alpha, samp_beta) w &lt;- exp(w - max(w)) q &lt;- w/sum(w) ## resample to create approximate sample from posterior inds &lt;- sample(seq(1,n,by = 1), size = m, replace = TRUE, prob = q) post_alpha &lt;- samp_alpha[inds] post_beta &lt;- samp_beta[inds] library(MASS) ## generate a contour plot for posterior den &lt;- kde2d(x=post_alpha, y=post_beta, n = 100) zlim &lt;- range(den$z) contour(den$x, den$y, den$z, col=&quot;grey10&quot;, xlim = c(0,1), ylim = c(0,1), levels = pretty(zlim, 10), lwd=1, xlab = expression(alpha), ylab = expression(beta)) Figure 2.9: Contour plot of posterior sample obtained using SIR for multivariate example "],["bernstein-von-mises-theorem.html", "Chapter 3 Bernstein-von Mises Theorem 3.1 Introduction 3.2 Theorem 3.3 Limitations", " Chapter 3 Bernstein-von Mises Theorem Augustine Wigle 3.1 Introduction The Bernstein-von Mises theorem (or BVM theorem) connects Bayesian and frequentist inference. In this workshop, we will state the theorem, talk about its importance, and touch on some of the required assumptions. We will work through some examples in R which let us visualize the theorem, and finally, we will talk about violations of the assumptions. This workshop is intended to introduce you to the theorem and encourage you to consider when it applies. It is not a rigourous treatment of the theorem - for those interested in the more technical details, we recommend Van der Vaart (2000) and Kleijn and Van der Vaart (2012). In this section we will give a brief review of Bayesian inference, the posterior distribution, and credible intervals. 3.1.1 Bayesian Inference Since this theorem applies to Bayesian models, we will give a quick review of Bayesian inference. In Bayesian inference, our state of knowledge about anything unknown is described by a probability distribution. Bayesian statistical conclusions about a parameter \\(\\theta\\) are made in terms of probabilistic statements conditional on the observed data \\(y\\). The distribution of interest is therefore \\(p(\\theta \\mid y)\\), the posterior distribution. We first specify a model which provides the joint probability distribution, that is, \\(p(\\theta, y) = p(\\theta)p(y\\mid \\theta)\\), where \\(p(\\theta)\\) is the prior distribution, which describes prior beliefs about the parameter(s) \\(\\theta\\), and \\(p(y\\mid \\theta)\\) is the data distribution of the likelihood. Then, Bayes’ theorem shows how to obtain the posterior distribution: \\[\\begin{equation*} p(\\theta \\mid y) = \\frac{p(\\theta)p(y\\mid \\theta)}{p(y)} \\end{equation*}\\] In most models, \\(p(\\theta\\mid y)\\) does not have a known parametric form and computational methods are required to overcome this problem, such as Markov Chain Monte Carlo (MCMC), or sampling-resampling techniques, covered in Chapter 2. In some special cases of likelihood and prior distribution combinations, the posterior can be determined analytically. These are called conjugate models. In the examples throughout this workshop, we will use conjugate models to avoid the need for fancy sampling techniques. Point estimates for \\(\\theta\\) can be derived from the posterior distribution, for example, by taking the posterior median or mean. Credible intervals are the Bayesian version of confidence intervals. A credible interval of credibility level \\(100\\times(1-\\alpha)\\) are defined as sets where the posterior probability of \\(\\theta\\) in the set is \\(100\\times(1-\\alpha)\\) and can be obtained in a variety of ways, such as by taking the lower and upper \\(\\alpha/2\\) quantiles of the posterior distribution. 3.2 Theorem A succinct statement of the theorem is as follows (for a more detailed and technical statement and proof, see Van der Vaart (2000)): ::: {.theorem #unnamed-chunk-12} Under certain assumptions, a posterior distribution converges to a multivariate normal distribution centred at the maximum likelihood estimate \\(\\hat \\theta\\) and with covariance matrix given by \\(n^{-1}I(\\theta_0)^{-1}\\) as the sample size \\(n \\rightarrow \\infty\\) , where \\(\\theta_0\\) is the true population parameter and \\(I(\\theta_0)\\) is the Fisher information matrix evaluated at \\(\\theta_0\\). ::: Or in other words, as you get more data, the posterior looks more and more like the sampling distribution of the MLE (if the assumptions are satisfied). 3.2.1 Importance The BVM theorem is useful because it provides a frequentist justification for Bayesian inference. The main takeaway of the theorem is that Bayesian inference is asymptotically correct from a frequentist perspective. In particular, Bayesian credible intervals are asymptotically confidence intervals. Another way to think about the theorem’s interpretation is that the influence of the prior disappears and the posterior becomes normal once you observe enough data. This is analogous to a pure frequentist approach where there is no prior information and the sampling distribution of the MLE becomes normal as you observe more and more data. 3.2.2 Required Assumptions Of course, for the theorem to hold, we require several assumptions to be satisfied. We are only going to touch on some of the more important conditions and we will not go into the technical details for this workshop (for technical details see Van der Vaart (2000)). Some of the more important assumptions are: The log-likelihood is smooth The MLE is consistent The prior distribution has non-zero density in a neighborhood of the true value \\(\\theta_0\\) The true parameter value is on the interior of the parameter space The model has a finite and fixed number of parameters We will discuss when some of these conditions may be violated and see some examples. We will start with an example where this theorem holds! 3.2.3 Example 1 - Normal-normal model Let’s consider a nice example: we observe \\(Y_1, \\dots, Y_n\\) from a \\(N(\\theta, 1)\\) distribution. We are interested in estimating \\(\\theta\\). Let’s also suppose that the true value of \\(\\theta\\) is 0. Bayesian Approach We will use a normal prior for \\(\\theta\\), that is, \\(\\theta \\sim N(0,1)\\). This is actually an example of a conjugate distribution and so there is an analytical solution for the posterior, which will make plotting the solution very convenient: \\[\\begin{equation*} \\theta \\mid Y_1, \\dots, Y_n \\sim N(\\frac{\\sum_{i=1}^n Y_i}{n+1}, \\frac{1}{n + 1}). \\end{equation*}\\] Frequentist Approach The MLE of \\(\\theta\\) is the sample mean, \\(\\hat\\theta = \\sum_{i=1}^nY_i/n\\) and the Fisher information is \\(1\\). The sampling distribution of \\(\\hat \\theta\\) is \\[\\begin{equation*} \\hat \\theta\\sim N(0, \\frac{1}{n}). \\end{equation*}\\] Let’s look at what happens as we observe more and more data in R. # Set true param value theta_true &lt;- 0 # Functions to calculate the posterior mean and sd post_mean &lt;- function(x) { n &lt;- length(x) sum(x)/(n + 1) } post_sd &lt;- function(x) { n &lt;- length(x) sqrt(1/(n + 1)) } # Function to calculate asymptotic sd for MLE mle_sd &lt;- function(x) { sqrt(1/length(x)) } # Generate some data of different sample sizes set.seed(2022) y_small &lt;- rnorm(10, mean = theta_true, sd = 1) y_med &lt;- rnorm(50, mean = theta_true, sd = 1) y_large &lt;- rnorm(100, mean = theta_true, sd = 1) # Set up Plotting x_vals &lt;- seq(-1, 1, by = 0.01) par(mfrow = c(3, 1)) # Plot the asymptotic distribution of MLE and posterior distribution # for n = 10 plot(x_vals, dnorm(x_vals, mean = mean(y_small), sd = mle_sd(y_small)), type = &quot;l&quot;, col = &quot;firebrick&quot;, ylab = &quot;y&quot;, xlab = &quot;x&quot;, main = &quot;n = 10&quot;, ylim = c(0, 1.3)) lines(x_vals, dnorm(x_vals, mean = post_mean(y_small), sd = post_sd(y_small)), col = &quot;navy&quot;) abline(v = 0, lty = 2) # Plot the asymptotic distribution of MLE and posterior distribution # for n = 50 plot(x_vals, dnorm(x_vals, mean = mean(y_med), sd = mle_sd(y_med)), type = &quot;l&quot;, col = &quot;firebrick&quot;, ylab = &quot;y&quot;, xlab = &quot;x&quot;, main = &quot;n = 50&quot;) lines(x_vals, dnorm(x_vals, mean = post_mean(y_med), sd = post_sd(y_med)), col = &quot;navy&quot;) abline(v = 0, lty = 2) # Plot the asymptotic distribution of MLE and posterior distribution # for n = 100 plot(x_vals, dnorm(x_vals, mean = mean(y_large), sd = mle_sd(y_large)), type = &quot;l&quot;, col = &quot;firebrick&quot;, ylab = &quot;y&quot;, xlab = &quot;x&quot;, main = &quot;n = 100&quot;) lines(x_vals, dnorm(x_vals, mean = post_mean(y_large), sd = post_sd(y_large)), col = &quot;navy&quot;) legend(&quot;bottomleft&quot;, legend = c(&quot;Posterior Distribution&quot;, &quot;MLE Sampling Distribution&quot;), col = c(&quot;navy&quot;, &quot;firebrick&quot;), lty = c(1, 1)) abline(v = 0, lty = 2) 3.2.4 Example 2 - Bernoulli-Beta Model In the previous example, we didn’t actually need Bernstein-von Mises to get asymptotic normality of the posterior, because the posterior distribution was already normal by definition regardless of how much data we had. Let’s look at another example where we can see the posterior getting more normal as we observe more data. Again, we will take advantage of conjugacy for computational convenience. This time, consider observing \\(Y_1, \\dots, Y_n\\) from \\(Bernoulli(p)\\), where the true value of \\(p\\) is 0.5. Bayesian Approach We will use a \\(Beta(1, 5)\\) distribution to take advantage of conjugacy. Then the posterior distribution for \\(p\\) is: \\[\\begin{equation*} p\\mid Y_1,\\dots, Y_n \\sim Beta(1+ \\sum_{i=1}^n Y_i, n+5 - \\sum_{i=1}^n Y_i) \\end{equation*}\\] Frequentist Approach The MLE \\(\\hat p = \\sum_{i=1}^n Y_i/n\\) and the Fisher information is \\(1/p(1-p)\\). The sampling distribution is then \\[\\begin{equation*} \\hat p \\sim N(0.5, \\frac{p(1-p)}{n}) \\end{equation*}\\] p_true &lt;- 0.5 # Functions to calculate the posterior parameters post_alpha &lt;- function(x) { 1 + sum(x) } post_beta &lt;- function(x) { n &lt;- length(x) n + 5 - sum(x) } # Function to calculate asymptotic sd for MLE mle_sd &lt;- function(p, x) { n &lt;- length(x) sqrt(p * (1 - p)/n) } # Generate some data of different sample sizes set.seed(2022) y_small &lt;- rbinom(10, size = 1, prob = p_true) y_med &lt;- rbinom(50, size = 1, prob = p_true) y_large &lt;- rbinom(200, size = 1, prob = p_true) # Set up Plotting x_vals &lt;- seq(0, 1, by = 0.01) par(mfrow = c(3, 1)) # Plot the asymptotic distribution of MLE and posterior distribution # for n = 10 plot(x_vals, dnorm(x_vals, mean = mean(y_small), sd = mle_sd(p_true, y_small)), type = &quot;l&quot;, col = &quot;firebrick&quot;, ylab = &quot;y&quot;, xlab = &quot;x&quot;, main = &quot;n = 10&quot;, ylim = c(0, 3)) lines(x_vals, dbeta(x_vals, shape1 = post_alpha(y_small), shape2 = post_beta(y_small)), col = &quot;navy&quot;) abline(v = p_true, lty = 2) # Plot the asymptotic distribution of MLE and posterior distribution # for n = 50 plot(x_vals, dnorm(x_vals, mean = mean(y_med), sd = mle_sd(p_true, y_med)), type = &quot;l&quot;, col = &quot;firebrick&quot;, ylab = &quot;y&quot;, xlab = &quot;x&quot;, main = &quot;n = 50&quot;) lines(x_vals, dbeta(x_vals, shape1 = post_alpha(y_med), shape2 = post_beta(y_med)), col = &quot;navy&quot;) abline(v = p_true, lty = 2) # Plot the asymptotic distribution of MLE and posterior distribution # for n = 100 plot(x_vals, dnorm(x_vals, mean = mean(y_large), sd = mle_sd(p_true, y_large)), type = &quot;l&quot;, col = &quot;firebrick&quot;, ylab = &quot;y&quot;, xlab = &quot;x&quot;, main = &quot;n = 200&quot;) lines(x_vals, dbeta(x_vals, shape1 = post_alpha(y_large), shape2 = post_beta(y_large)), col = &quot;navy&quot;) abline(v = p_true, lty = 2) legend(&quot;bottomleft&quot;, legend = c(&quot;Posterior Distribution&quot;, &quot;MLE Sampling Distribution&quot;), col = c(&quot;navy&quot;, &quot;firebrick&quot;), lty = c(1, 1)) 3.3 Limitations We do need to be careful with applying Bernstein-von Mises to make sure important assumptions are satisfied! Several of these examples were motivated by this blog post by Dan Simpson which I found entertaining and thought-provoking (Simpson 2017). Here are some examples of when important assumptions of BVM might be violated: The log-likelihood being smooth may be violated in situations were we want to integrate out nuisance parameters, because this can cause spikes in the log likelihood. Our model may not have a fixed number of parameters, for example, in a multi-level model where observing more data may require including more categories and therefore more parameters. Some models have infinite-dimensional parameters, such as models which use non-parametric effects to model unknown functions. The prior may give zero density to the true parameter value - see example 3 \\(\\theta_0\\) may be on the boundary of the parameter space - see example 4 The MLE may not be consistent when the log-likelihood is multimodal, such as in some mixture models. Some other situations where the MLE may not be consistent arise when other assumptions are violated, like the number of parameters increasing with \\(n\\) or the true parameter value being on the boundary of the parameter space. 3.3.1 Other thoughts on consistency Other considerations around consistency which I found interesting are raised by Dan Simpson in his blog post mentioned above. When we consider consistency, we need to consider how the data were collected, and consider what it means to have independent replicates of the same experiment. A lot of datasets are observational, and so there is no guarantee that the data can actually be used to give a consistent estimator of the parameter we want to estimate, regardless of how many times we conduct the experiment. Similarly, collecting a lot of data can take a long time, and over time, the underlying process that we are trying to study may change. This will also present a challenge for consistency. 3.3.2 Example 3 - Prior has zero density at \\(\\theta_0\\) An example of this would be using a uniform prior for a standard deviation and choosing an upper bound which is less than the true standard deviation. For this talk, we won’t do an example where the prior gives 0 density to the true value because for these models we would need to use some computational tools tog et the posterior distribution, but we can look at what happens when the prior gives very little density to the true value. Let’s return to the first example, where we have collected data from a normal distribution with known variance 1 and we want to estimate its mean. In the first example, we used a prior centred at 0 with variance 1, and the true value happened to be 0. What if the true mean is actually 1000? The prior \\(N(0,1)\\) is now very informative, and will give very little density (but still non-zero) to the true parameter value. Now we have: Bayesian Approach Recall that the posterior is: \\[\\begin{equation*} \\theta \\mid Y_1, \\dots, Y_n \\sim N(\\frac{\\sum_{i=1}^n Y_i}{n+1}, \\frac{1}{n + 1}). \\end{equation*}\\] Frequentist Approach The asymptotic distribution of the MLE is: \\[\\begin{equation*} \\hat \\theta\\sim N(1000, \\frac{1}{n}). \\end{equation*}\\] # Set true param value theta_true &lt;- 1000 # Functions to calculate the posterior mean and sd post_mean &lt;- function(x) { n &lt;- length(x) sum(x)/(n + 1) } post_sd &lt;- function(x) { n &lt;- length(x) sqrt(1/(n + 1)) } # Function to calculate asymptotic sd for MLE mle_sd &lt;- function(x) { sqrt(1/length(x)) } # Generate some data of different sample sizes set.seed(2022) y_small &lt;- rnorm(10, mean = theta_true, sd = 1) y_med &lt;- rnorm(100, mean = theta_true, sd = 1) y_large &lt;- rnorm(1000, mean = theta_true, sd = 1) # Set up Plotting x_vals &lt;- seq(985, 1001, by = 0.01) par(mfrow = c(3, 1)) # Plot the asymptotic distribution of MLE and posterior distribution # for n = 10 plot(x_vals, dnorm(x_vals, mean = mean(y_small), sd = mle_sd(y_small)), type = &quot;l&quot;, col = &quot;firebrick&quot;, ylab = &quot;y&quot;, xlab = &quot;x&quot;, main = &quot;n = 10&quot;, ylim = c(0, 1.3)) lines(x_vals, dnorm(x_vals, mean = post_mean(y_small), sd = post_sd(y_small)), col = &quot;navy&quot;) abline(v = theta_true, lty = 2) # Plot the asymptotic distribution of MLE and posterior distribution # for n = 100 plot(x_vals, dnorm(x_vals, mean = mean(y_med), sd = mle_sd(y_med)), type = &quot;l&quot;, col = &quot;firebrick&quot;, ylab = &quot;y&quot;, xlab = &quot;x&quot;, main = &quot;n = 100&quot;) lines(x_vals, dnorm(x_vals, mean = post_mean(y_med), sd = post_sd(y_med)), col = &quot;navy&quot;) abline(v = theta_true, lty = 2) # Plot the asymptotic distribution of MLE and posterior distribution # for n = 1000 plot(x_vals, dnorm(x_vals, mean = mean(y_large), sd = mle_sd(y_large)), type = &quot;l&quot;, col = &quot;firebrick&quot;, ylab = &quot;y&quot;, xlab = &quot;x&quot;, main = &quot;n = 1000&quot;) lines(x_vals, dnorm(x_vals, mean = post_mean(y_large), sd = post_sd(y_large)), col = &quot;navy&quot;) abline(v = theta_true, lty = 2) legend(&quot;bottomleft&quot;, legend = c(&quot;Posterior Distribution&quot;, &quot;MLE Sampling Distribution&quot;), col = c(&quot;navy&quot;, &quot;firebrick&quot;), lty = c(1, 1)) 3.3.3 Example 4 - True parameter value is on the boundary Let’s return to the Bernoulli example from before, but this time we will see what happens when the probability is on the boundary, that is, the true value of \\(p = 1\\). Bayesian Approach Recall the posterior distribution for \\(p\\) with a \\(Beta(1, 5)\\) prior is: \\[\\begin{equation*} p\\mid Y_1,\\dots, Y_n \\sim Beta(1+ \\sum_{i=1}^n Y_i, n+5 - \\sum_{i=1}^n Y_i) \\end{equation*}\\] Frequentist Approach The MLE \\(\\hat p = \\sum_{i=1}^n Y_i/n\\) and the Fisher information is \\(1/p(1-p)\\). The sampling distribution is then \\[\\begin{equation*} \\hat p \\sim N(1, 0). \\end{equation*}\\] Note that the variance of the MLE’s sampling distribution is now zero, so in other words, it is just a point mass at \\(p=1\\)! Let’s see what happens to the posterior. p_true &lt;- 1 # Functions to calculate the posterior mean and sd post_alpha &lt;- function(x) { 1 + sum(x) } post_beta &lt;- function(x) { n &lt;- length(x) n + 5 - sum(x) } # Function to calculate asymptotic sd for MLE mle_sd &lt;- function(p, x) { n &lt;- length(x) sqrt(p * (1 - p)/n) } # Generate some data of different sample sizes set.seed(2022) y_small &lt;- rbinom(10, size = 1, prob = p_true) y_med &lt;- rbinom(100, size = 1, prob = p_true) y_large &lt;- rbinom(1000, size = 1, prob = p_true) # Set up Plotting x_vals &lt;- seq(0, 1, by = 0.01) par(mfrow = c(3, 1)) # Plot the asymptotic distribution of MLE and posterior distribution # for n = 10 plot(x_vals, dbeta(x_vals, shape1 = post_alpha(y_small), shape2 = post_beta(y_small)), col = &quot;navy&quot;, type = &quot;l&quot;, ylab = &quot;y&quot;, xlab = &quot;x&quot;, main = &quot;n = 10&quot;, ylim = c(0, 3)) abline(v = p_true, lty = 2, col = &quot;firebrick&quot;) # Plot the asymptotic distribution of MLE and posterior distribution # for n = 50 plot(x_vals, dbeta(x_vals, shape1 = post_alpha(y_med), shape2 = post_beta(y_med)), col = &quot;navy&quot;, type = &quot;l&quot;, ylab = &quot;y&quot;, xlab = &quot;x&quot;, main = &quot;n = 100&quot;) abline(v = p_true, lty = 2, col = &quot;firebrick&quot;) # Plot the asymptotic distribution of MLE and posterior distribution # for n = 100 plot(x_vals, dbeta(x_vals, shape1 = post_alpha(y_large), shape2 = post_beta(y_large)), col = &quot;navy&quot;, type = &quot;l&quot;, ylab = &quot;y&quot;, xlab = &quot;x&quot;, main = &quot;n = 1000&quot;) abline(v = p_true, lty = 2, col = &quot;firebrick&quot;) legend(&quot;bottomleft&quot;, legend = c(&quot;Posterior Distribution&quot;, &quot;MLE Sampling Distribution&quot;), col = c(&quot;navy&quot;, &quot;firebrick&quot;), lty = c(1, 1)) "],["variational-inference.html", "Chapter 4 Variational Inference 4.1 Introduction 4.2 Example: Mixture of Gaussians 4.3 Example: Stochastic Variational Inference using Pyro in Python 4.4 Takeaways", " Chapter 4 Variational Inference Meixi Chen 4.1 Introduction Variational inference (VI) is an inference technique that is commonly used to approximate an intractable quantity such as a probability density. In the following, we introduce two scenarios where the VI is often used, one in a frequentist setting and the other in a Bayesian setting. 4.1.1 Frequentist Setting The frequentist example is adapted from the one in Chen (2020). Consider the following IID observations \\(Y_1,\\ldots,Y_n\\). Now assume that each \\(Y_i\\) is accompanied with an unobserved latent variable \\(Z_i\\). That is, the complete data is \\((Y_1,Z_1),\\ldots,(Y_n,Z_n)\\), but we only observe the incomplete data \\(Y_1,\\ldots,Y_n\\). Assume that we know the parametric model for the complete data is \\(p(y, z\\mid \\theta)\\), and our interest lies in estimating \\(\\theta\\). One way to do it is to maximize the observed log-likelihood \\(\\ell(\\theta\\mid y_1,\\ldots,y_n) = \\sum_{i=1}^n \\log p(y_i\\mid\\theta)\\), where \\[\\begin{equation} \\tag{4.1} p(y_i\\mid\\theta)=\\int p(y_i, z_i\\mid \\theta) \\ dz_i. \\end{equation}\\] However, this integral is typically difficult to compute. Many techniques exist to deal with the problem of (4.1) (e.g., MCMC, Laplace approximation, EM). The VI is one such method to solve this problem, which writes \\[\\begin{equation} \\tag{4.2} \\begin{aligned} p(y\\mid \\theta) &amp;= \\int p(y, z\\mid \\theta)\\ dz\\\\ &amp;= \\int \\frac{p(y, z\\mid \\theta)}{\\color{red}{q(z\\mid \\omega)}}\\color{red}{q(z\\mid \\omega)} \\ dz\\\\ &amp;= \\mathbb{E}_{Z}\\left[\\frac{p(y, z\\mid \\theta)}{\\color{red}{q(z\\mid \\omega)}}\\right] \\end{aligned} \\end{equation}\\] where \\(Z\\sim q(z\\mid \\omega)\\) and \\(q(\\cdot \\mid\\omega)\\) is called the variational distribution and typically has an easy form (e.g. Normal distribution). All possible candidate variational distributions form the variational family \\(\\mathcal{Q}=\\{q(\\cdot\\mid\\omega): \\ \\omega \\in \\Omega\\}\\). Given (4.2), we can compute the observed log-likelihood as \\[\\begin{equation} \\tag{4.3} \\begin{aligned} \\ell(\\theta\\mid y) &amp;= \\log p(y\\mid \\theta)\\\\ &amp;= \\log \\mathbb{E}_{Z}\\left[\\frac{p(y, z\\mid \\theta)}{q(z\\mid \\omega)}\\right]\\\\ &amp;\\ge \\mathbb{E}_{Z}\\left[\\log\\frac{p(y, z\\mid \\theta)}{q(z\\mid \\omega)}\\right] \\text{ using Jensen&#39;s inequality}\\\\ &amp;=\\mathbb{E}_{Z}(\\log p(y, z\\mid \\theta))-\\mathbb{E}_{Z}(\\log q(z\\mid \\omega))\\\\ &amp;:= \\mathrm{ELBO}(\\omega,\\theta\\mid y) := \\mathrm{ELBO}(q) \\end{aligned} \\end{equation}\\] where \\(\\mathrm{ELBO}(q)\\) is known as the Evidence Lower Bound. Now, instead of maximizing \\(\\ell(\\theta\\mid y_1,\\ldots,y_n)\\), we can maxmize the ELBO via any numerical optimization algorithm, i.e., \\[\\begin{equation} \\tag{4.4} (\\hat{\\omega}, \\hat{\\theta}) = \\underset{\\omega,\\theta}{\\mathrm{argmax}}\\frac{1}{n}\\sum_{i=1}^n \\mathrm{ELBO}(\\omega,\\theta\\mid y_i). \\end{equation}\\] Important notes The parametric form of \\(q(\\cdot \\mid \\omega)\\) is up to the modeler. A common choice is Normal. When \\(q(z \\mid \\omega)\\) is multivariate, i.e., \\(z, \\omega\\in\\mathbb{R}^n\\), it is common to use the mean-field variational family \\(q(z\\mid \\omega)=\\prod_{i=1}^n q(z_i\\mid \\omega_i)\\). The VI estimator \\(\\hat{\\theta}_{\\mathrm{VI}}\\) generally does not converge to the MLE because \\(\\hat{\\theta}_{\\mathrm{VI}}\\) depends on the choice of \\(q(\\cdot \\mid \\omega)\\). Uncertainty assessment One way to assess the uncertainty of the VI estimator \\(\\tilde{\\theta}_{\\mathrm{VI}}\\) is via bootstrapping. Let \\((Y_1^{(b)}, \\ldots,Y_n^{(b)})\\) be the \\(b\\)-th bootstrap sample from the original dataset, for \\(b=1,\\ldots, B\\). Given the bootstrap sample, we can compute the bootstrap VI estimate \\(\\hat{\\theta}_{\\mathrm{VI}}^{(b)}\\). After repeating the above procedure for \\(B\\) times, we obtain \\(B\\) bootstrap VI estimates: \\(\\hat{\\theta}_{\\mathrm{VI}}^{(1)}, \\ldots, \\hat{\\theta}_{\\mathrm{VI}}^{(B)}\\). The bootstrap estimates can be used to calculate the uncertainty of the original bootstrap estimator. 4.1.2 Bayesian Setting In Bayesian statistics, the VI is often used as an alternative to the traditional MCMC sampling method to estimate the posterior distributions \\(p(\\theta\\mid y)\\). Recall that, according to the Bayes’ rule, the posterior distribution is written as \\[p(\\theta\\mid y) = \\frac{p(y\\mid \\theta)p(\\theta)}{\\color{red}{p(y)}} = \\frac{p(y\\mid \\theta)p(\\theta)}{\\int p(y\\mid \\theta) p(\\theta)\\ d\\theta},\\] where \\(p(y\\mid \\theta)\\) is the likelihood taking a known form and \\(p(y)\\) is a constant. Similar to the problem in (4.1), the integral \\(p(y)=\\int p(y\\mid \\theta) p(\\theta)\\ d\\theta\\) is typically intractable, which makes calculating the posterior difficult. What the VI does in this case is to find a variational distribution \\(q(\\theta \\mid \\omega)\\) such that it is close enough to the posterior distribution of interest \\(p(\\theta\\mid y)\\). How do we know a distribution is “close enough” to another? The answer is using the Kullback-Leibler divergence \\(\\mathrm{KL}(Q\\Vert P)\\), which measures how different the probability distribution \\(Q\\) is from the reference distribution \\(P\\). Therefore, the VI method looks for \\(q(\\theta \\mid \\omega)\\) that minimizes \\(\\mathrm{KL}\\big(q(\\theta\\mid \\omega) \\Vert p(\\theta\\mid y)\\big)\\), i.e., \\[q^*(\\theta \\mid \\omega) = \\underset{q(\\theta\\mid\\omega)}{\\mathrm{argmin}} \\mathrm{KL}\\big(q(\\theta\\mid \\omega) \\Vert p(\\theta\\mid y)\\big).\\] The KL divergence is written as \\[\\begin{equation} \\tag{4.5} \\begin{aligned} \\mathrm{KL}\\big(q(\\theta\\mid \\omega) \\Vert p(\\theta\\mid y)\\big) &amp;= \\mathbb{E}_{q(\\cdot \\mid \\omega)}[\\log q(\\theta\\mid \\omega)]-\\mathbb{E}_{q(\\cdot \\mid \\omega)}[\\log p(\\theta\\mid y)]\\\\ &amp;= \\mathbb{E}_{q(\\cdot \\mid \\omega)}[\\log q(\\theta\\mid \\omega)]-\\mathbb{E}_{q(\\cdot \\mid \\omega)}\\left[\\log \\frac{p(\\theta, y)}{p(y)}\\right]\\\\ &amp;= \\underbrace{\\mathbb{E}_{q(\\cdot \\mid \\omega)}[\\log q(\\theta\\mid \\omega)]-\\mathbb{E}_{q(\\cdot \\mid \\omega)}[\\log p(\\theta, y)]}_{-\\mathrm{ELBO(q)}} + \\underbrace{\\color{red}{\\log p(y)}}_{\\text{constant}} \\end{aligned} \\end{equation}\\] Noting that \\(\\log p(y)\\) is a constant, minimizing the KL divergence (4.5) is in fact equivalent to minimizing the nonconstant part of (4.5): \\[\\mathbb{E}_{q(\\cdot \\mid \\omega)}[\\log q(\\theta\\mid \\omega)]-\\mathbb{E}_{q(\\cdot \\mid \\omega)}[\\log p(\\theta, y)].\\] Then we notice that it is in fact the negative of the ELBO, so minimizing (4.5) is in turn equivalent to maximizing the ELBO: \\[\\mathrm{ELBO}(q) = \\mathbb{E}_{q(\\cdot \\mid \\omega)}[\\log p(\\theta, y)]-\\mathbb{E}_{q(\\cdot \\mid \\omega)}[\\log q(\\theta\\mid \\omega)].\\] Therefore, we can estimate \\(\\omega\\) as \\[\\hat{\\omega} = \\underset{\\omega}{\\mathrm{argmax}} \\ \\mathrm{ELBO}(q) = \\underset{\\omega}{\\mathrm{argmax}} \\ \\mathrm{ELBO}(\\omega\\mid y).\\] Finally, the posterior of interest is approximated by \\[p(\\theta\\mid y) \\approx q(\\theta \\mid \\hat{\\omega}).\\] Important note Similar to the frequentist setting, we need to pick the distributional form of \\(q(\\cdot \\mid \\omega)\\). The posterior distribution obtained this way is an approximation rather than the truth, whereas MCMC methods guarantees that the Monte Carlo samples converge to the true posterior distribution. Takeaway Only use the VI to estimate the posterior if the dimension of the posterior is too high for sampling, or the model is too complex for MCMC methods to run within a reasonable amount of time (within a day). 4.2 Example: Mixture of Gaussians This example is taken from a famous VI tutorial paper by Blei, Kucukelbir, and McAuliffe (2017). Consider the following Gaussian mixture model: \\[\\begin{equation} \\tag{4.6} \\begin{aligned} \\mu_k &amp;\\sim \\mathcal{N}(0, \\sigma^2), \\ \\ &amp; k=1,\\ldots, K,\\\\ c_i &amp;\\sim \\mathrm{Categorical}(1/K, \\ldots, 1/K), \\ \\ &amp; i=1,\\ldots, n,\\\\ x_i|c_i, \\boldsymbol{\\mu} &amp;\\sim \\mathcal{N}(c_i^T\\boldsymbol{\\mu}, 1), \\ \\ &amp; i=1,\\ldots,n, \\end{aligned} \\end{equation}\\] where the prior variance \\(\\sigma^2\\) is known, and \\(c_i=(0,0,\\ldots,1,\\ldots,0)\\) is one-hot encoding. The parameters of interest are the mean parameters \\((\\mu_1,\\ldots,\\mu_K)\\) and the cluster parameters \\((c_1,\\ldots,c_n)\\). We propose to use the mean-field variational family \\(q(\\boldsymbol{\\mu}, \\boldsymbol{c})=\\prod_{i=1}^Kq(\\mu_k)\\prod_{i=1}^nq(c_i)\\), where \\[\\begin{equation} \\tag{4.7} \\begin{aligned} q(\\mu_k\\mid m_k, s_k^2) &amp;= \\mathcal{N}(m_k, s_k^2), \\ \\ &amp; k=1,\\ldots, K\\\\ q(c_i\\mid \\phi_i) &amp;= \\mathrm{Categorical}(\\phi_{i1}, \\ldots, \\phi_{iK}), \\ \\ &amp; i=1,\\ldots n. \\end{aligned} \\end{equation}\\] Again, let \\(\\omega=(m, s, \\phi)\\) denote all the variational parameters, and let \\(\\theta=(\\mu_1,\\ldots,\\mu_K, c_1,\\ldots,c_n)\\) denote all the parameters of interest. Then the ELBO is written as \\[\\begin{equation} \\tag{4.8} \\begin{aligned} \\mathrm{ELBO}(q) &amp;= \\color{blue}{\\mathbb{E}_{q(\\cdot \\mid \\omega)}[\\log p(\\theta, x)]}-\\color{purple}{\\mathbb{E}_{q(\\cdot \\mid \\omega)}[\\log q(\\theta \\mid \\omega)]}\\\\ &amp;= \\color{blue}{\\sum_{k=1}^K \\mathbb{E}[\\log p(\\mu_k); m_k, s_k^2]} \\\\ &amp; \\ \\ \\ \\ \\ \\ \\color{blue}{+\\sum_{i=1}^n \\left(\\mathbb{E}[\\log p(c_i); \\phi_i] + \\mathbb{E}[\\log p(x_i\\mid c_i, \\mu); \\phi, m, s^2]\\right)} \\\\ &amp; \\ \\ \\ \\ \\ \\ \\color{purple}{-\\sum_{i=1}^n\\mathbb{E}[\\log q(c_i\\mid \\phi_i)] - \\sum_{k=1}^K \\mathbb{E}[\\log q(\\mu_k\\mid m_k, s_k^2)]} \\end{aligned} \\end{equation}\\] Note that \\(\\mathbb{E}[a ; b]\\) specifies the quantity of interest \\(a\\) depends on the variational parameter \\(b\\). Each of the above term can be computed in closed form (but omitted here). The next question is, how do we maximize the ELBO? 4.2.1 Coordinate Ascent Mean-Field Variational Inference The coordinate ascent variational inference (CAVI) is commonly used to solve this optimization problem. It is particularly convenient for the mean-field variational family. Results: (Blei, Kucukelbir, and McAuliffe 2017) Let the full conditional of \\(\\theta_j\\) be \\(p(\\theta_j\\mid \\boldsymbol{\\theta}_{-j},x)\\). When all variational distributions \\(q(\\theta_{\\ell})\\) for \\(\\ell\\neq j\\) are fixed, the optimal \\(q(\\theta_j)\\) is proportionate to the exponentiated expected log complete conditional: \\[\\begin{equation} \\tag{4.9} q^*(\\theta_j) \\propto \\exp\\left\\{\\mathbb{E}_{-j}[\\log p(\\theta_j\\mid\\boldsymbol{\\theta}_{-j}, x)]\\right\\} \\end{equation}\\] This result is used to formulate the CAVI algorithm as follows. Algorithm: CAVI while the ELBO has not converged do for \\(j\\in\\{1,\\ldots,m\\}\\) do Set \\(q_j(\\theta_j) \\propto \\exp\\left\\{\\mathbb{E}_{-j}[\\log p(\\theta_j\\mid\\boldsymbol{\\theta}_{-j}, x)]\\right\\}\\) end Compute \\(\\mathrm{ELBO}(q)\\) end Return \\(q(\\boldsymbol{\\theta})\\) For the mixture of Gaussians example, Blei, Kucukelbir, and McAuliffe (2017) has derived the full conditionals and computed the updating rules for all variational parameters: \\[\\begin{equation} \\tag{4.10} \\begin{aligned} \\phi_{ik} &amp;\\propto \\exp\\{\\mathbb{E}[\\mu_k;m_k,s_k^2]x_i - \\mathbb{E}[\\mu_k^2;m_k,s_k^2]/2\\} \\ \\ \\text{(normalize afterwards)}\\\\ m_k &amp;= \\frac{\\sum_{i}^n \\phi_{ik}x_i}{1/\\sigma^2 + \\sum_{i=1}^n \\phi_{ik}} \\\\ s_k^2 &amp;= \\frac{1}{1/\\sigma^2 +\\sum_{i=1}^n \\phi_{ik}} \\end{aligned} \\end{equation}\\] Then the full CAVI algorithm for the Gaussian mixture model is given below. Algorithm: CAVI for Gaussian mixture model while the ELBO has not converged do for \\(i\\in\\{1,\\ldots,n\\}\\) do Set \\(\\phi_{ik} \\propto \\exp\\{\\mathbb{E}[\\mu_k;m_k,s_k^2]x_i - \\mathbb{E}[\\mu_k^2;m_k,s_k^2]/2\\}\\) end for \\(k\\in\\{1,\\ldots,K\\}\\) do Set \\(m_k = \\frac{\\sum_{i}^n \\phi_{ik}x_i}{1/\\sigma^2 + \\sum_{i=1}^n \\phi_{ik}}\\) Set \\(s_k^2 = \\frac{1}{1/\\sigma^2 +\\sum_{i=1}^n \\phi_{ik}}\\) end Compute \\(\\mathrm{ELBO}(\\boldsymbol{m},\\boldsymbol{s}^2, \\boldsymbol{\\phi})\\) end return \\(q(\\mu_k \\mid m_k, s_k^2)\\) and \\(q(c_k\\mid \\phi_i)\\) 4.3 Example: Stochastic Variational Inference using Pyro in Python As we have seen in the previous example, to use CAVI, we need to compute (and code up) the ELBO and the full conditionals in closed-form. This will be a pain when we have more complex models. When we perform statistical inference in practice, we typically only need to provide a model form. The model can be as simple as a formula such as Response ~ var1 + var2 in the glm package, or in a more complicated form such as the Stan language when we perform MCMC sampling using the rstan package. We almost never needed to derive any math! We can do the same with the VI. That is, we only need to provide the model formulation, and then let the software do the tedious derivation for us. However, there does not exist an automatic and versatile R package for VI. Therefore, we will look at Pyro, a probabilistic programming language (PPL) written in Python, which is a very convenient interface for implementing VI for complex models. Note that Pyro is supported by the popular deep learning framework Pytorch on the backend, so models written in Pyro can be easily extended to incorporate neural network architectures. We will go over a simpler example provided by the Pyro team itself. All the following code is in Python and originally provided here. To install pyro, simply run pip install pyro-ppl in your terminal. First, we import the required packages and set up some system parameters. import os import logging import time import torch import numpy as np import pandas as pd import seaborn as sns # for plotting import matplotlib.pyplot as plt # for plotting import pyro import pyro.distributions as dist import pyro.distributions.constraints as constraints pyro.enable_validation(True) pyro.set_rng_seed(1) logging.basicConfig(format=&#39;%(message)s&#39;, level=logging.INFO) plt.style.use(&#39;default&#39;) The dataset we will look at in this example studies the relationship between geography and national income. Specifically, we will examine how this relationship is different for nations in Africa and outside of Africa. It was found that, outside of Africa, bad geography is associated with lower GPD, whereas the relationship is reversed in Africa. Response variable rgdppc_2000: Real GPD per capita in 2000 (will be log transformed for analysis). Predictor rugged: The Terrain Ruggedness Index which measures the topographic heterogeneity of a nation. Predictor cont_africa: Whether the nation is in Africa. The following plots show the relationships between log GDP and ruggedness index for Non-African nations and African nations, respectively. DATA_URL = &quot;https://d2hg8soec8ck9v.cloudfront.net/datasets/rugged_data.csv&quot; data = pd.read_csv(DATA_URL, encoding=&quot;ISO-8859-1&quot;) df = data[[&quot;cont_africa&quot;, &quot;rugged&quot;, &quot;rgdppc_2000&quot;]] df = df[np.isfinite(df.rgdppc_2000)] df[&quot;rgdppc_2000&quot;] = np.log(df[&quot;rgdppc_2000&quot;]) # log transform the highly-skewed GDP data train = torch.tensor(df.values, dtype=torch.float) is_cont_africa, ruggedness, log_gdp = train[:, 0], train[:, 1], train[:, 2] fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 6), sharey=True) african_nations = df[df[&quot;cont_africa&quot;] == 1] non_african_nations = df[df[&quot;cont_africa&quot;] == 0] sns.scatterplot(x=non_african_nations[&quot;rugged&quot;], y=non_african_nations[&quot;rgdppc_2000&quot;], ax=ax[0]) ax[0].set(xlabel=&quot;Terrain Ruggedness Index&quot;, ylabel=&quot;log GDP (2000)&quot;, title=&quot;Non African Nations&quot;) sns.scatterplot(x=african_nations[&quot;rugged&quot;], y=african_nations[&quot;rgdppc_2000&quot;], ax=ax[1]) ax[1].set(xlabel=&quot;Terrain Ruggedness Index&quot;, ylabel=&quot;log GDP (2000)&quot;, title=&quot;African Nations&quot;) plt.show() A simple model to capture the relationship is \\[Y = \\alpha + \\beta_aX_a + \\beta_rX_r + \\beta_{ar} X_aX_r +\\epsilon, \\ \\ \\epsilon\\sim\\mathcal{N}(0,\\sigma^2)\\] where \\(Y\\) is the log GDP, \\(X_a\\) is an indicator for whether the nation is in Africa, \\(X_r\\) is the ruggedness index, and \\(\\epsilon\\) is the noise term. This model is defined as freq_model in the following, where the parameters and the observations are specified using pyro.param and pyro.sample, respectively. A cool feature of Pyro is that its render_model() function allows us to visualize the model. def freq_model(is_cont_africa, ruggedness, log_gdp=None): a = pyro.param(&quot;a&quot;, lambda: torch.randn(())) b_a = pyro.param(&quot;bA&quot;, lambda: torch.randn(())) b_r = pyro.param(&quot;bR&quot;, lambda: torch.randn(())) b_ar = pyro.param(&quot;bAR&quot;, lambda: torch.randn(())) sigma = pyro.param(&quot;sigma&quot;, lambda: torch.ones(()), constraint=constraints.positive) mean = a + b_a * is_cont_africa + b_r * ruggedness + b_ar * is_cont_africa * ruggedness with pyro.plate(&quot;data&quot;, len(ruggedness)): return pyro.sample(&quot;obs&quot;, dist.Normal(mean, sigma), obs=log_gdp) pyro.render_model(freq_model, model_args=(is_cont_africa, ruggedness, log_gdp), render_distributions=True, render_params=True, filename=&quot;freq_model.png&quot;); We can also define a Bayesian version of it called bayes_model as follows. This model simply replaces pyro.param with pyro.sample, so that the parameters are viewed as random variables following some (prior) distributions. Similarly, we call render_model() to visualize it. Between the Bayesian model and the frequentist model, we will use the former for demonstration here. def bayes_model(is_cont_africa, ruggedness, log_gdp=None): a = pyro.sample(&quot;a&quot;, dist.Normal(0., 10.)) b_a = pyro.sample(&quot;bA&quot;, dist.Normal(0., 1.)) b_r = pyro.sample(&quot;bR&quot;, dist.Normal(0., 1.)) b_ar = pyro.sample(&quot;bAR&quot;, dist.Normal(0., 1.)) sigma = pyro.sample(&quot;sigma&quot;, dist.Uniform(0., 10.)) mean = a + b_a * is_cont_africa + b_r * ruggedness + b_ar * is_cont_africa * ruggedness with pyro.plate(&quot;data&quot;, len(ruggedness)): return pyro.sample(&quot;obs&quot;, dist.Normal(mean, sigma), obs=log_gdp) pyro.render_model(bayes_model, model_args=(is_cont_africa, ruggedness, log_gdp), render_distributions=True, filename=&quot;bayes_model.png&quot;); In the context of Pyro, the variational distribution is called a “guide”. To specify the variational family we want, we need to define a guide program as follows. The code is very similar to that of the model. The custom guide we define below uses mean-field variational inference, i.e., all parameters have independent variational distributions in the form of Gaussian. \\(q(\\alpha\\mid \\mu_\\alpha, \\sigma^2_\\alpha) = \\mathcal{N}(\\mu_\\alpha, \\sigma^2_\\alpha)\\) \\(q(\\beta_a\\mid \\mu_{\\beta_a}, \\sigma^2_{\\beta_a}) = \\mathcal{N}(\\mu_{\\beta_a}, \\sigma^2_{\\beta_a}a)\\) \\(q(\\beta_r\\mid \\mu_{\\beta_r}, \\sigma^2_{\\beta_r}) = \\mathcal{N}(\\mu_{\\beta_r}, \\sigma^2_{\\beta_r})\\) \\(q(\\beta_{ar}\\mid \\mu_{\\beta_{ar}}, \\sigma^2_{\\beta_{ar}}) = \\mathcal{N}(\\mu_{\\beta_{ar}}, \\sigma^2_{\\beta_{ar}})\\) \\(q(\\sigma^2\\mid \\mu_\\sigma, \\sigma^2_\\sigma) = \\mathcal{N}(\\mu_\\sigma, \\sigma^2_\\sigma)\\) def custom_guide(is_cont_africa, ruggedness, log_gdp=None): a_loc = pyro.param(&#39;a_loc&#39;, lambda: torch.tensor(0.)) a_scale = pyro.param(&#39;a_scale&#39;, lambda: torch.tensor(1.), constraint=constraints.positive) sigma_loc = pyro.param(&#39;sigma_loc&#39;, lambda: torch.tensor(1.), constraint=constraints.positive) weights_loc = pyro.param(&#39;weights_loc&#39;, lambda: torch.randn(3)) weights_scale = pyro.param(&#39;weights_scale&#39;, lambda: torch.ones(3), constraint=constraints.positive) a = pyro.sample(&quot;a&quot;, dist.Normal(a_loc, a_scale)) b_a = pyro.sample(&quot;bA&quot;, dist.Normal(weights_loc[0], weights_scale[0])) b_r = pyro.sample(&quot;bR&quot;, dist.Normal(weights_loc[1], weights_scale[1])) b_ar = pyro.sample(&quot;bAR&quot;, dist.Normal(weights_loc[2], weights_scale[2])) sigma = pyro.sample(&quot;sigma&quot;, dist.Normal(sigma_loc, torch.tensor(0.05))) return {&quot;a&quot;: a, &quot;b_a&quot;: b_a, &quot;b_r&quot;: b_r, &quot;b_ar&quot;: b_ar, &quot;sigma&quot;: sigma} pyro.render_model(custom_guide, model_args=(is_cont_africa, ruggedness, log_gdp), render_params=True, filename=&quot;custom_guide.png&quot;); To implement variational inference in Pyro, we use its stochastic variational inference functionality pyro.infer.SVI(), which takes in four arguments: Model: bayes_model Guide (Variational distribution): custom_guide ELBO: We do not need to compute the explicit ELBO. It can be defined by calling pyro.infer.Trace_ELBO(), and this will automatically compute ELBO under the hood for us given model and guide. Optimizer: Any optimizer can do, but a popular choice is the Adam optimizer given by pyro.optim.Adam(). Adam is a gradient-based optimization algorithm that computes adaptive learning rates for different parameters. The plot below shows the ELBO loss (negative ELBO) as a function of the step index. We see that the optimization procedure has converged after about 500 steps. pyro.clear_param_store() auto_guide = pyro.infer.autoguide.AutoNormal(bayes_model) adam = pyro.optim.Adam({&quot;lr&quot;: 0.02}) elbo = pyro.infer.Trace_ELBO() svi = pyro.infer.SVI(bayes_model, auto_guide, adam, elbo) losses = [] vi_start = time.time() for step in range(1000): loss = svi.step(is_cont_africa, ruggedness, log_gdp) losses.append(loss) if step % 100 == 0: logging.info(&quot;Elbo loss: {}&quot;.format(loss)) ## Elbo loss: 694.9404839277267 ## Elbo loss: 524.3822101950645 ## Elbo loss: 475.668176651001 ## Elbo loss: 399.99088364839554 ## Elbo loss: 315.23277366161346 ## Elbo loss: 254.76771301031113 ## Elbo loss: 248.237025141716 ## Elbo loss: 248.42669039964676 ## Elbo loss: 248.46450036764145 ## Elbo loss: 257.41463327407837 vi_end = time.time() plt.figure(figsize=(5, 2)) plt.plot(losses) plt.xlabel(&quot;SVI step&quot;) plt.ylabel(&quot;ELBO loss&quot;) plt.show() Below is the time taken to optimize the ELBO using the VI method. print(vi_end-vi_start) ## 5.405915021896362 Then we can print out the estimated variational parameters (\\(\\boldsymbol{\\omega}\\)), which are the mean and variance of each variational Normal distribution. for name, value in pyro.get_param_store().items(): print(name, pyro.param(name).data.cpu().numpy()) ## AutoNormal.locs.a 9.173145 ## AutoNormal.scales.a 0.07036691 ## AutoNormal.locs.bA -1.847466 ## AutoNormal.scales.bA 0.14070092 ## AutoNormal.locs.bR -0.1903212 ## AutoNormal.scales.bR 0.044044245 ## AutoNormal.locs.bAR 0.35599768 ## AutoNormal.scales.bAR 0.07937442 ## AutoNormal.locs.sigma -2.205863 ## AutoNormal.scales.sigma 0.06052672 Given the variational parameters, we can now sample from the variational distributions of the parameters of interest. The variational distributions can be treated as approximated posterior distributions of these parameters. In particular, we can look at the slopes (log GDP versus ruggedness index) for Non-African nations and African nations. Non-African nations: slope = \\(\\beta_r\\) African nations: slope = \\(\\beta_r+\\beta_{ar}\\) We will take 800 samples from \\(q(\\beta_{r}\\mid \\hat{\\mu}_{\\beta_{r}}, \\hat{\\sigma}^2_{\\beta_{r}})\\) and \\(q(\\beta_{ar}\\mid \\hat{\\mu}_{\\beta_{ar}}, \\hat{\\sigma}^2_{\\beta_{ar}})\\). The histograms of the samples are plotted side-by-side below. with pyro.plate(&quot;samples&quot;, 800, dim=-1): samples = auto_guide(is_cont_africa, ruggedness) gamma_within_africa = samples[&quot;bR&quot;] + samples[&quot;bAR&quot;] gamma_outside_africa = samples[&quot;bR&quot;] fig = plt.figure(figsize=(10, 6)) plt.hist(gamma_within_africa.detach().numpy(), label=&quot;Arican nations&quot;); plt.hist(gamma_outside_africa.detach().numpy(), label=&quot;Non-Arican nations&quot;); fig.suptitle(&quot;Density of Slope : log(GDP) vs. Terrain Ruggedness&quot;); plt.xlabel(&quot;Slope of regression line&quot;) plt.legend() plt.show() Finally, we can compare the VI to MCMC (NUTS, to be specific). from pyro.infer import MCMC, NUTS mcmc = MCMC(NUTS(bayes_model), num_samples=1000, warmup_steps=1000, disable_progbar=True) mc_start = time.time() mcmc.run(is_cont_africa, ruggedness, log_gdp) mc_end = time.time() mc_samples = mcmc.get_samples() mcmc.summary() ## ## mean std median 5.0% 95.0% n_eff r_hat ## a 9.19 0.14 9.19 8.95 9.42 537.96 1.00 ## bA -1.86 0.23 -1.86 -2.22 -1.48 497.48 1.00 ## bAR 0.36 0.13 0.36 0.12 0.55 563.94 1.00 ## bR -0.19 0.08 -0.19 -0.31 -0.05 507.07 1.00 ## sigma 0.95 0.05 0.95 0.86 1.03 648.89 1.00 ## ## Number of divergences: 0 Below is the time taken to draw 2000 MCMC samples (including 1000 warmups). We can see MCMC is almost an order of magnitude slower than the VI. print(mc_end-mc_start) ## 38.20520734786987 We can also plot the histograms of the MCMC samples of the two slopes. Compared to the histograms obtained using the VI, these don’t look much different. mc_slope_within_africa = mc_samples[&quot;bR&quot;] + mc_samples[&quot;bAR&quot;] mc_slope_outside_africa = mc_samples[&quot;bR&quot;] fig = plt.figure(figsize=(10, 6)) plt.hist(mc_slope_within_africa.detach().numpy(), label=&quot;Arican nations&quot;); plt.hist(mc_slope_outside_africa.detach().numpy(), label=&quot;Non-Arican nations&quot;); fig.suptitle(&quot;MCMC Estimated Density of Slope : log(GDP) vs. Terrain Ruggedness&quot;); plt.xlabel(&quot;Slope of regression line&quot;) plt.legend() plt.show() 4.4 Takeaways In a Bayesian example, the VI is an order of magnitude faster than MCMC while giving similar results. However, the example uses a very simple Normal-Normal model, and the variational distribution employed is also Normal. This could be why the VI performs so well. When the true posterior is multi-model or asymmetric, it is unlikely that using a mean-field Gaussian variational distribution will give as accurate results. Since the VI is an optimization-based algorithm, it is possible that the ELBO has not converged to a local minimum (in more complex modelling settings). Diagnostics are required for this. The VI is known to underestimate model uncertainty. It is not surprising since it is an approximate inference method. Unless your model is so complicated (e.g., multiple hierarchies, neural nets) that traditional frequentist fitting procedures (lme4, glm) break down or MCMC methods take forever, the VI would not be your first choice. "],["smoothing-techniques.html", "Chapter 5 Smoothing Techniques 5.1 Introduction 5.2 Kernel Smoothing Methods 5.3 Smoothing Spline", " Chapter 5 Smoothing Techniques Jie Jian Smoothing techniques are frequently used to reduce the variability of the observational data and enhance prediction accuracy, while still providing an adequate fit. In this workshop, we will introduce two popular smoothing methods, kernel smoothing and smoothing splines, from their formulations to the computations. Each method has a smoothing parameter that controls the amount of roughness. We will demonstrate how to choose the optimal tuning parameters from the perspective of variance-bias trade-off. Some examples of how to apply the two methods will be provided. Part of the codes and materials come from Hastie et al. (2009) and James et al. (2013). 5.1 Introduction A fundamental problem in statistical learning is to use a data set \\(\\{ (x_i,y_i) \\}_{i=1}^n\\) to learn a function \\(f(\\cdot)\\) such that \\(f(x_i)\\) fit the data \\(y_i\\) well, so we can use the estimated function \\(f\\) on the new data \\(x\\) to predict the future outcome \\(y\\). In a statistical analysis, we have two sources of information, data and models. Data is “unbiased” but it contains noise, and models involve more constraints so contain less noise but introduce bias. In between the two extremes, we can use smoothing techniques to extract more information from the data and meanwhile control the variance. 5.1.0.1 Example: Boston house value Housing values and other information about Boston suburbs. The variable lstat measures the percentage of individuals with lower socioeconomic status. The variable medv records median house values for 506 neighborhoods around Boston. library(MASS) y = Boston$medv x = Boston$lstat y.lab = &quot;Median Property Value&quot; x.lab = &quot;Lower Status (%)&quot; plot(x, y, cex.lab = 1.1, col = &quot;darkgrey&quot;, xlab = x.lab, ylab = y.lab, main = &quot;&quot;, bty = &quot;l&quot;) library(splines) library(FNN) xrange &lt;- extendrange(x) Xorder &lt;- order(x) xnew &lt;- seq(min(xrange), max(xrange), length.out = 500) fit1 &lt;- knn.reg(x, y = y, k = 5) fit2 &lt;- lm(y ~ bs(x, degree = 3, df = 4)) ypred2 &lt;- predict(fit2, newdata = data.frame(x = xnew)) plot(x, y, col = &quot;grey80&quot;, pch = 19, cex = 0.5, main = &quot;Fits with different \\&quot;smoothness\\&quot;&quot;, xlab = x.lab, ylab = y.lab) lines(x[Xorder], fit1$pred[Xorder], col = adjustcolor(&quot;steelblue&quot;, 0.5), lwd = 2) lines(xnew, ypred2, col = &quot;darkgreen&quot;, lwd = 2) legend(max(xrange) - 15, max(extendrange(y)), legend = c(&quot;5 nearest neighbours&quot;, &quot;B-spline df=4&quot;), lty = c(1, 1), lwd = 2, col = c(&quot;steelblue&quot;, &quot;darkgreen&quot;)) 5.2 Kernel Smoothing Methods A natural way to achieve smoothness is to utilize the local information of data to compute the fit, so that the the estimated model would not change too much over the data. In the kernel smoothing method, for any data point \\(x_i\\), the value of the function at the point \\(f(x_0)\\) is estimated using the combination of nearby observations. The contribution of each observation \\(x_i\\) is calculated using a weight function, defined as Kernel \\(K_\\lambda(x_0,x_i)\\), related to the distance between \\(x_0\\) and \\(x_i\\). The parameter \\(\\lambda\\) controls the width of the neighborhood. 5.2.1 Local linear regression Locally weighted regression solves a separate weighted least squares problem at each target point \\(x_0\\): \\[\\min\\limits_{\\alpha(x_0),\\beta(x_0)} \\sum_{i=1}^N K_\\lambda (x_0,x_i) [y_i - \\alpha(x_0)-\\beta(x_0)x_i]^2.\\] The estimated function at each target point \\(x_0\\) is \\(\\hat{f}(x_0)=\\hat{\\alpha} (x_0)+\\hat{\\beta}(x_0) x_i\\). We can only use it to fit at a single point. The smoothing parameter \\(\\lambda\\) in the kernel function, which determines the width of the local neighborhood, has to be determined. Large \\(\\lambda\\) implies lower variance (averages over more observations) but higher bias (we essentially assume the true function is constant within the window). Kernel functions (p194, Hastie et al. 2009) Locally weighted linear regression (p195, Hastie et al. 2009) 5.2.1.1 Computation For each target point \\(x_0\\), \\[\\begin{bmatrix} \\hat{\\alpha} [x_0] \\\\ \\hat{\\beta} [x_0] \\end{bmatrix}=\\arg\\min\\limits_{\\alpha,\\beta} \\sum_{i=1}^N K_\\lambda (x_0,x_i)\\cdot (y_i-\\alpha-(x_i-x_0) \\beta)^2,\\] if we define an \\(n-\\)by\\(-n\\) weighting matrix \\[W_h(x_0) = diag(K_\\lambda(x_0,x_1),\\cdots,K_\\lambda(x_0,x_n)) ,\\] then we can rewrite this optimization as \\[\\begin{bmatrix} \\hat{\\alpha} [x_0] \\\\ \\hat{\\beta} [x_0] \\end{bmatrix}=\\arg\\min\\limits_{\\alpha,\\beta} \\| W_h(x_0) \\cdot (Y- [\\mathbf{1}_n\\ \\ X_0] \\begin{bmatrix}\\alpha \\\\ \\beta \\end{bmatrix})^2\\|^2_2,\\] which is a OLS optimization \\[\\begin{bmatrix} \\hat{\\alpha}[x_0] \\\\ \\hat{\\beta} [x_0]\\end{bmatrix}=([\\mathbf{1}_n\\ \\ X_0]&#39; W_h(x_0) [\\mathbf{1}_n\\ \\ X_0])^{-1}([\\mathbf{1}_n\\ \\ X_0]&#39;W_h(x_0) Y).\\] 5.2.1.2 One-line implementation in R: loess loess function in R fits a LOcally wEighted Sum of Squares estimate. The local neighborhood determined by either span: the portion of points in the local neighborhood enp.target: effective degrees of freedom The kernel is Tukey’s tri-cube. Degree can be 0, 1, or 2. The default is a local quadratic. Up to 4 predictors. fit1 = loess(y ~ x, span = 0.05) fit2 = loess(y ~ x, span = 0.3) plot(x, y, col = &quot;grey80&quot;, pch = 19, cex = 0.5, main = &quot;loess with different spans&quot;, xlab = x.lab, ylab = y.lab) lines(x[Xorder], predict(fit1, x[Xorder]), col = adjustcolor(&quot;steelblue&quot;, 0.5), lwd = 2) lines(x[Xorder], predict(fit2, x[Xorder]), col = &quot;darkgreen&quot;, lwd = 2) legend(max(xrange) - 15, max(extendrange(y)), legend = c(&quot;span=0.05&quot;, &quot;span=0.3&quot;), lty = c(1, 1), lwd = 2, col = c(&quot;steelblue&quot;, &quot;darkgreen&quot;)) 5.2.1.3 Bias and vairance \\(\\text{bias}(\\hat{\\alpha} [x_0])=O(h^2)\\) and \\(var (\\hat{\\alpha}[x_0])=O_p (\\frac{1}{nh^d}),\\) where \\(h\\) is the bandwidth and \\(d\\) is the dimensionality of \\(x\\). Theoretically, we can pick the bandwidth \\(h\\) in some optimal sense such that \\(h=\\arg\\max\\limits_{h} (c_1 h^4 + c_2 \\frac{1}{nh^2})\\). Therefore, the asymptotic rate for the bandwidth \\(h\\) is \\(h=O(n^{-1/(d+4)})\\). [derivation skipped] 5.2.2 Tuning Parameter (bandwidth) Selection The tuning parameter \\(\\lambda\\) of the kernel \\(K_\\lambda\\) controls the width of the averaging window. - if the window is narrow, \\(\\hat{f}(x_0)\\) is an average of a small number of \\(y_i\\) close to \\(x_0\\), and its variance will be relatively large while the bias will tend to be small. - if the window is wide, the variance of \\(\\hat{f}(x_0)\\) will be small and the bias will be higher. Choosing the bandwidth is a bias-variance trade-off. Leave-one-out cross validation (LOOCV) For each \\(i=1,\\cdots,n\\), compute the estimator \\(\\hat{f}_\\lambda^{(-i)} (x)\\), where \\(\\hat{f}_\\lambda^{(-i)} (x)\\) is comupted without using observation \\(i\\). The estimated MSE is given by \\[\\hat{\\text{MSE}}(\\lambda)=\\frac{1}{n} \\sum_i (y_i-\\hat{f}_\\lambda^{(-i)} (x_i))^2.\\] [derivation needed] 5.2.3 Extension and example: Local logistic regression We can extend the local kernel smoothing method to \\[\\max \\sum_{i=1}^N K_\\lambda (x_0,x_i) l(y_i,x_i^T \\beta(x_0)),\\] where \\(l(y_i,x_i^T \\beta(x_0))\\) is replaced by the specific log-likelihood. For example, in the local logistic regression where we consider logistic regression with a single quantitative input \\(X\\). The local log-odds at a target \\(x_0\\) is \\[\\log \\frac{\\mathbb{P}(Y=1|X=x_0)}{\\mathbb{P}(Y=0|X=x_0)}=\\alpha(x_0)+\\beta(x_0)x_i.\\] The objective function that we need to maximize is adjusted as \\[\\max \\sum_{i=1}^N K_\\lambda (x_0,x_i) [y_i \\log p(x_i) + (1-y_i) \\log (1-p(x_i))].\\] 5.3 Smoothing Spline When fitting a function \\(f(\\cdot)\\) to a set of data, the first thing that we want is to find some function that fits the observed data well: that is, we want the residual sum of squares which measures the goodness of fit, \\(RSS=\\sum_{i=1}^{n} (y_i - f(x_i))^2\\), to be small. If this is our only repuirement, then we can even make RSS to zero by simply interpolating all observational data (a non-linear function that passes through exactly every data point). Such flexible function would overfit the data and lead to a large variance. Therefore, we not only require the RSS to be small, but also the function to be smooth. To achieve the smoothness of the estimated function, some penalty can be added to control the variability in \\(f(x)\\). Considering the two requirements, we are seeking a function \\(f(x)\\) that minimizes \\[\\underbrace{\\sum_{i=1}^{n} (y_i-f(x_i))^2}_\\text{Model fit} + \\lambda \\underbrace{\\int f&#39;&#39;(x)^2 dx}_\\text{Penalty term}\\] where \\(\\lambda\\) is a nonnegative tuning parameter. The function \\(f(x)\\) is known as a smoothing spline. Model fit: different from the general linear regression problem where the function \\(f(x)\\) is linear and we only need to estimate the coefficients of the predictors, here we minimize the objective function with respect to \\(f(x)\\). Penalty term: the second term penalizes curvature in the function. Tuning parameter: when \\(\\lambda =0\\) we get a wiggly non-linear function, which has a high variance and low bias; As \\(\\lambda\\) increases the estimated function will be smoother; When \\(\\lambda \\rightarrow \\infty\\), \\(f&#39;&#39;\\) will be zero everywhere which leads to a linear model \\(f(x)=\\beta_0+\\beta_1 x\\), which has a low variance but high bias. 5.3.1 Computation From the perspective of functional space Gu (2013), the smoothing spline is the function \\(f_\\lambda\\) that minimizes the spline functional \\[E[f]=\\sum_{i=1}^{n} (y_i-f(x_i))^2 + \\lambda \\int f&#39;&#39;(x)^2 dx\\] in the Sobolev space \\(W_2 (\\mathbb{R})\\) of functions with square integrable second derivative: \\[f_\\lambda = \\arg \\min_{f\\in W_2 (\\mathbb{R})} E[f].\\] Sobolev space is an infinite-dimensional function space, where the second term is defined. It can be shown that the solution to \\(\\min E[f]\\) is an explicit, finite-dimensional, unique minimizer which is a natural cubic spline with knots at the unique values of the predictor values \\(x_i\\). Since the solution is a natural spline, we can write it as \\(f(x)=\\sum_{i=1}^n N_j (x) \\theta_j\\), where the \\(N_j (x)\\) are an N-dimensional set of basis functions for representing the family of natural splines. The criterion reduces to \\[RSS(\\theta,\\lambda)=(y-N\\theta)^\\top (y-N\\theta) + \\lambda \\theta^\\top \\Omega_N \\theta,\\] where \\(\\{ N \\}_{ij}=N_j (x_i)\\) and \\(\\{ \\Omega_N \\}_{jk}=\\int N&#39;&#39;_j (t) N&#39;&#39;_k (t) dt\\). The solution is easily seen to be \\[\\hat{\\theta} = (N^\\top N + \\lambda \\Omega_N)^{-1} N^T y.\\] The fitted smoothing spline is given by \\[\\hat{f} (x) = \\sum_{j=1}^N N_j (x) \\hat{\\theta}_j.\\] Although natural splines provide a basis for smoothing splines, it is computationally more convenient to operate in the larger space of unconstrained B-splines. We write \\(f(x)=\\sum_{i=1}^{N+4}B_i(x)\\theta_i\\) (just to replace the basis functions from natural cubic splines to B-splines). 5.3.2 Tuning Parameter Selection The tuning parameter controls the tradeoff between smoothness of the estimate and fidelity to the data. In statistical terms, it controls the tradeoff between bias and variance. The optimal value of \\(\\lambda\\) has to be estimated from the data, usually by cross-validation or generalized cross-validation. Cross validation Generalized cross-validation Usually degrees of freedom refer to the number of free parameters, such as the number of coefficients fit in a polynomial or cubic spline. Although a smoothing spline has \\(n\\) parameters and hence \\(n\\) nominal degrees of freedom, these \\(n\\) parameters are heavily constrained or shrunk down. We use the effective degrees of freedom as a measure of the flexibility of the smoothing spline. We can write \\[\\hat{f}_\\lambda = S_\\lambda y,\\] where \\(\\hat{f}\\) is the solution for a given \\(\\lambda\\) and \\(S_\\lambda\\) is an \\(n-\\)by\\(-n\\) matrix. The effective degrees of freedom is defined as the sum of the diagonal elements of the matrix \\(S_\\lambda\\): \\[df_\\lambda = \\sum_{i=1}^n \\{S_\\lambda \\}_{ii}.\\] The generalized cross validation (GCV) for such linear smoother is \\[GCV(\\hat{f})=\\frac{1}{n} \\sum_{i=1}^n (\\frac{y_i - \\hat{f}(x_i)}{1-tr (S)/n})^2.\\] [derivation needed] library(splines) smooth1 = smooth.spline(x, y, df = 3) smooth2 = smooth.spline(x, y, cv = TRUE) par(mfrow = c(1, 2)) plot(x, y, cex.lab = 1.1, col = &quot;darkgrey&quot;, xlab = x.lab, ylab = y.lab, main = &quot;Smoothing Spline (3 df)&quot;, bty = &quot;l&quot;) lines(smooth1, lwd = 2, col = &quot;brown&quot;) plot(x, y, cex.lab = 1.1, col = &quot;darkgrey&quot;, xlab = x.lab, ylab = y.lab, main = &quot;Smoothing Spline (CV)&quot;, bty = &quot;l&quot;) lines(smooth2, lwd = 2, col = &quot;darkorange&quot;) 5.3.3 Extension and example: Nonparametric logistic regression The smoothing spline problem we talked above is posed in a regression setting, as the model fit part is \\(\\sum_{i=1}^{n} (y_i-f(x_i))^2\\). The technique can be extended to other problems, as long as we adjust the model fit term in the objective function to some likelihood-based formula. Here, we consider logistic regression with a single quantitative input \\(X\\). \\[\\log \\frac{\\mathbb{P}(Y=1|X=x)}{\\mathbb{P}(Y=0|X=x)}=f(x).\\] Our target is to fit a smooth function \\(f(x)\\) to the logit, so that the conditional probability \\(\\mathbb{P}(Y=1|x)=\\frac{e^{f(x)}}{1+e^{f(x)}}\\) is also smooth, which can be used for classification or risk measurement. In order to incorporate the smoothness penalty, the penalized log-likelihood criterion is \\[l(f;\\lambda)=\\sum_{i=1}^{N} [y_i \\log p(x_i) + (1-y_i) \\log (1-p(x_i))]-\\frac{1}{2} \\lambda \\int \\{f&#39;&#39;(t) \\}^2 dt.\\] It also can be shown that the solution to the optimization problem is the natural spline, so we can also express the function as \\(f(x)=\\sum_{i=1}^N N_j (x) \\theta_j\\) where \\(N\\) is the matrix of natural spline, or further it can be replaced by the B-spline for the same reason we have in the regression setting (approximation). 5.3.4 Take-home note Smoothing techniques can incorporate with a variety of statistical methods. We introduced two methods: Local linear regression: fit locally using some kernel function to measure the weight of each data point. \\[\\max \\sum_{i=1}^N K_\\lambda (x_0,x_i) l(y_i,x_i^T \\beta(x_0))\\] Smoothing spline: add a penalty term in the objective function to prevent the function that needs to be estimated from not being smooth. \\[\\max \\sum_{i=1}^N l(x_i,y_i) -\\lambda P(f)\\] "],["references.html", "Chapter 6 References", " Chapter 6 References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
