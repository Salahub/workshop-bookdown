[["pooled-p-values.html", "Chapter 8 Pooled p-values 8.1 A motivating example 8.2 Pooling \\(p\\)-values 8.3 Adjusting for dependence", " Chapter 8 Pooled p-values Chris Salahub 8.1 A motivating example Let’s say we have collected a bunch of genetic data and would like to identify which genes are related to some observed trait. We’ll start by defining some terms to make describing things easier. DNA: genetic information that is passed between generations and is stored in several big molecules made up of many small molecules called nucleotides genome: all the DNA of an organism gene: a segment of DNA that gives the blueprint for a particular protein chromosomes: one of the several big molecules making up the genome marker: a sequence of nucleotides measured at a known position in DNA sex: the creation of an organism in a new generation by recombining DNA from two organisms in the previous generation recombination: when DNA is changed slightly by segments of DNA literally crossing each other and switching or chromosomes being inherited independently, pictorially: centiMorgans: an additive measure of distance on DNA, for two markers which have probability \\(q\\) of being separated during sex the distance in centiMorgans is \\(d = 50 \\ln \\frac{1}{1 - 2q}\\) So, when we talk about genetic data, we mean markers at known positions on chromosomes which ideally span the whole genome of a sample of organisms. We pair this data with a physical measurement - in humans examples include height, eye colour, hair colour, or the presence of a disease - to determine how much physical variation can be explained by genetic variation. Physical traits that are linked to genetics are either Mendelian or non-Mendelian (you can guess which came first). Mendelian traits have variation explained by one (or very few) gene(s), take eye colour which is mostly down to the gene OCA2. Non-Mendelian traits, like height, have variation which cannot be attributed to one (or a handful of) genetic region(s) alone. To identify which measured markers are related to a physical trait, a host of significance tests are typically computed to assay the impact of each. While this is not optimal compared to a linear model, it is frequently the only option for genetic studies which collect hundred of thousands of markers for only a few dozen individuals. Difficulty fitting a model practically means we must filter these markers first. Before even pointing our finger at a subset of genes, however, we might want to save time by asking whether there is anything interesting happening at all in this collection of tests. That is where pooled \\(p\\)-values come in. 8.1.1 Some “real” data Our ideal data set would involve humans. Genomics mostly strives to better understand the genetic sources of human diseases and traits to help create better medicine and understand ourselves more fully. Unfortunately, human data is pretty hard to come by ethically, and mice data is a lot easier to find. The code below pulls some that is freely available from Mouse Genome Informatics. ## this script provides a means of downloading mouse genomic data from the ## mouse genome database (MGD), an online repository of data on mouse ## genetics ## all the data is stored in text files at mgiUrl &lt;- &quot;https://www.informatics.jax.org/downloads/reports/&quot; ## we&#39;ll be looking at data from mgiPName &lt;- &quot;MGI_JAX_BSB_Panel.rpt&quot; ## and for reference we&#39;ll also need the marker lists in mgiMNames &lt;- c(&quot;MRK_List1.rpt&quot;, &quot;MRK_List2.rpt&quot;) ## this function reads in MGI text files, which are basically organized ## as a tab-separated data frame with extra information readMGIrpt &lt;- function(file) { raw &lt;- scan(file, what = character(), sep = &quot;\\n&quot;) # most basic R input leg &lt;- which(raw == &quot;Legend:&quot;) # identify the legend rows lenHead &lt;- leg + 4 # the +4 is from inspection of the legends if (length(leg) == 0) { # separate legend leg &lt;- which(grepl(&quot;^CHR&quot;, raw))[1] lenHead &lt;- leg } desc &lt;- paste(raw[1:lenHead], collapse = &quot;\\n&quot;) # data description dat &lt;- raw[(lenHead+1):length(raw)] # actual data ## each row in the data is associated with a reference, the study ## or paper that characterized or collected the corresponding ## observation, we want to separate these refPos &lt;- regexec(&quot;\\\\tJ\\\\:[0-9]+(?:, J\\\\:[0-9]+){0,4}&quot;, dat) refs &lt;- sapply(regmatches(dat, refPos), # extract references function(el) { if (length(el) == 0) { &quot;&quot; } else gsub(&quot;\\\\t&quot;, &quot;&quot;, el)}) data &lt;- unlist(regmatches(dat, refPos, invert = TRUE)) # remove refs mat &lt;- do.call(rbind, strsplit(data[data != &quot;&quot;], &quot;\\\\t&quot;)) # regularize rwnms &lt;- mat[1, -(1:3)] # animal numbers/ids colnms &lt;- mat[-1, 3] # symbol field colDesc &lt;- mat[-1, 1:3] # symbol details colnames(colDesc) &lt;- c(&quot;chr&quot;, &quot;mgiid&quot;, &quot;symbol&quot;) # informative names data &lt;- t(mat[-1,-(1:3)]) # observed values rownames(data) &lt;- rwnms # final data formatting colnames(data) &lt;- colnms list(summary = desc, # return everything in a big list markers = data.frame(colDesc, ref = refs[-1]), data = as.data.frame(data)) } ## this function looks at the marker reference data and processes it into ## a form easier for future analysis readMGIlists &lt;- function(fileList = paste0(mgiUrl, mgiMNames)) { lists &lt;- lapply(fileList, scan, # similar input to above what = character(), sep = &quot;\\n&quot;) lists &lt;- lapply(lists, gsub, # replace end of line tabs with periods pattern = &quot;\\t$&quot;, replacement= &quot;\\t.&quot;) splits &lt;- lapply(lists, strsplit, split = &quot;\\t&quot;) # split by tabs colnms &lt;- splits[[1]][[1]] # column names data &lt;- do.call(rbind, # covert into one big matrix lapply(splits, function(splt) do.call(rbind, splt[-1]))) colnames(data) &lt;- colnms as.data.frame(data) } ## using indices and a reference table, process centiMorgan positions processcMs &lt;- function(inds, tab) { sel &lt;- tab[inds] # take indices from table sel[grepl(&quot;syntenic&quot;, sel)] &lt;- &quot;Inf&quot; suppressWarnings(as.numeric(sel)) # warnings by design } ## this function goes through a processed panel from the above functions ## and drops the measurements that have un filterPanelBycM &lt;- function(panel, locs) { locOrd &lt;- order(locs) # order position toKeep &lt;- is.finite(locs[locOrd]) # drop NAs and Infs outMrk &lt;- data.frame(panel$markers[locOrd[toKeep],], cMs = locs[locOrd[toKeep]]) outMrk &lt;- outMrk[order(outMrk$chr),] # order chromosome list(summary = panel$summary, markers = outMrk, data = panel$data[, outMrk$symbol]) } ## pull the marker reference mgiMarkers &lt;- readMGIlists() # descriptions of all markers ## pull the panel mgiPanel &lt;- readMGIrpt(paste0(mgiUrl, mgiPName)) ## match the marker names back to the marker reference mgiPanel.mrkr &lt;- match(names(mgiPanel$data), mgiMarkers$`Marker Symbol`) ## get cM positions mgiPanel.cMs &lt;- processcMs(mgiPanel.mrkr, tab = mgiMarkers$`cM Position`) ## Infs indicate markers localized to a chromosome but without a ## known cM position, while NAs indicate markers missing from the ## reference file or marked as unknown there ## filter the panels by markers with known positions mgiFiltered &lt;- filterPanelBycM(mgiPanel, mgiPanel.cMs) ## and convert the data to numeric values mgiFiltered$data &lt;- as.data.frame(lapply(mgiFiltered$data, function(mrkr) as.numeric(mrkr== &quot;b&quot;))) ## remove intermediate data rm(list = c(&quot;mgiMarkers&quot;, &quot;mgiPanel.cMs&quot;, &quot;mgiPanel.mrkr&quot;, &quot;mgiPanel&quot;)) summary(mgiFiltered) ## Length Class Mode ## summary 1 -none- character ## markers 5 data.frame list ## data 1598 data.frame list dim(mgiFiltered$data) ## [1] 94 1598 In the end, after reading and cleaning the data, we have 94 mice each measured at 1598 marker locations. We are still in the unfortunate position of lacking physical measurements for any of the 94 mice on this panel, but can still generate pseudo \\(p\\)-values. For Mendelian traits, we can pick a marker and generate a trait based solely on its value, while non-Mendelian traits will require the selection of numerous locations which may not contribute equally. For now, let’s keep this as simple as possible by ensuring independence. Chromosomes assort independently, so if we take one marker from each chromosome we should have independent markers. ## match in this context will give the index of the first marker on ## each chromosome mgiIndep &lt;- mgiFiltered$data[, match(c(1:19, &quot;X&quot;), mgiFiltered$markers$chr)] ## inspect dependence structure image(cor(mgiIndep)) To simulate a Mendelian trait, we’ll simply add noise to the first marker. For non-Mendelian inheritance, let’s just take the average of all markers and add noise. set.seed(2314) traitMendel &lt;- mgiIndep[, 1] + rnorm(94, sd = 0.3) plot(density(traitMendel)) # what mendelian traits look like traitNonMend &lt;- apply(mgiIndep[, 1:20], 1, mean) + rnorm(94, sd = 0.3) plot(density(traitNonMend)) # what non-mendelian traits look like Finally, we can compute \\(p\\)-values for each marker and the traits based on the correlation between them. ## based on the mendelian trait (pMendel &lt;- apply(mgiIndep, 2, function(col) cor.test(traitMendel, col)$p.value)) ## D1Mit475 D2Mit312 D3Mit60 D4Mit149 D5Mit331 D6Mit86 ## 8.140813e-28 5.562233e-01 5.701328e-01 7.242746e-01 7.675526e-01 3.105180e-01 ## D7Mit21 Fcer2a D9Mit186 D10Mit166 D11Mit1 D12Mit37 ## 5.065030e-01 5.402953e-01 6.714437e-01 4.083773e-01 4.271063e-02 2.917104e-01 ## D13Mit158 D14Mit179 D15Mit12 D16Mit32 Tfb1m D18Mit66 ## 5.489663e-01 2.092645e-01 4.023976e-01 1.364540e-01 1.034232e-01 5.719812e-01 ## D19Mit32 DXMit26 ## 7.726304e-01 8.591501e-01 ## based on the non-mendelian trait (pnonMendel &lt;- apply(mgiIndep, 2, function(col) cor.test(traitNonMend, col)$p.value)) ## D1Mit475 D2Mit312 D3Mit60 D4Mit149 D5Mit331 D6Mit86 D7Mit21 ## 0.84662803 0.13598050 0.09371634 0.19260532 0.07979980 0.40937430 0.69903954 ## Fcer2a D9Mit186 D10Mit166 D11Mit1 D12Mit37 D13Mit158 D14Mit179 ## 0.04886178 0.14942358 0.14839598 0.04176343 0.15557356 0.42105667 0.57307268 ## D15Mit12 D16Mit32 Tfb1m D18Mit66 D19Mit32 DXMit26 ## 0.82419580 0.51590067 0.10878820 0.18622075 0.09006944 0.81802465 ## some null cases pNull &lt;- replicate(1000, apply(mgiIndep, 2, function(col) cor.test(runif(94), col)$p.value)) head(t(pNull)) ## D1Mit475 D2Mit312 D3Mit60 D4Mit149 D5Mit331 D6Mit86 D7Mit21 ## [1,] 0.90978137 0.9069532 0.35006885 0.52959032 0.7380976 0.7379150 0.7759258 ## [2,] 0.71128190 0.4672418 0.40962358 0.55101467 0.2559132 0.9183818 0.3258728 ## [3,] 0.95942070 0.5427160 0.04764853 0.06797646 0.7925067 0.4583663 0.8189367 ## [4,] 0.72433189 0.2469022 0.37670242 0.79942407 0.7073759 0.4569672 0.5144509 ## [5,] 0.09880001 0.5690093 0.04219049 0.67880376 0.5211135 0.2129442 0.5705933 ## [6,] 0.47118092 0.1661241 0.87974774 0.85674033 0.9537631 0.7554583 0.9164587 ## Fcer2a D9Mit186 D10Mit166 D11Mit1 D12Mit37 D13Mit158 D14Mit179 ## [1,] 0.6541013 0.892077786 0.0941390 0.9816285 0.58747481 0.96585574 0.10030964 ## [2,] 0.7740573 0.928080700 0.5653199 0.9420153 0.60334296 0.44579138 0.02940274 ## [3,] 0.2356859 0.830787629 0.7194775 0.3474443 0.72685240 0.95146071 0.68194085 ## [4,] 0.2079808 0.008370045 0.1754181 0.6098431 0.03384509 0.85023276 0.01777558 ## [5,] 0.2412231 0.034974937 0.8137640 0.8319588 0.64930667 0.06533281 0.23257463 ## [6,] 0.6922345 0.695358501 0.9634527 0.1120413 0.93632232 0.82351369 0.35253150 ## D15Mit12 D16Mit32 Tfb1m D18Mit66 D19Mit32 DXMit26 ## [1,] 0.9908900 0.3714854 0.5952756 0.76844887 0.6071990 0.72313619 ## [2,] 0.7466969 0.9263798 0.8664269 0.61143617 0.3156817 0.06558521 ## [3,] 0.7392575 0.6634759 0.7311247 0.68270873 0.3402068 0.45887828 ## [4,] 0.7151886 0.6484924 0.6415287 0.68611425 0.8145712 0.11268825 ## [5,] 0.9612551 0.1987970 0.8847019 0.93284250 0.5656709 0.55879076 ## [6,] 0.9654213 0.8425012 0.7445170 0.06411416 0.3992616 0.49837159 8.2 Pooling \\(p\\)-values Whether inheritance is Mendelian, non-Mendelian, or something else, our selection of markers tested against a trait gives a collection of \\(M=20\\) \\(p\\)-values from 20 hypothesis tests, and we’d like to know if further investigation is warranted for any of them. Directly, we want to test the null hypothesis \\[H_0 = \\cap_{i = 1}^M H_{0i}\\] where \\(H_{0i}\\) is the hypothesis of “no association” the \\(i\\)th marker. Another way of thinking about this is by considering the null distributions of our \\(M\\) \\(p\\)-values \\(p_1, \\dots, p_M\\) if there is nothing going on. In this case, we’d expect \\[H_0 = p_1, p_2, \\dots, p_M \\overset{\\mathrm{iid}}{\\sim} U\\] where \\(U = Unif(0,1)\\). The basic idea of pooled \\(p\\)-values is to think of a function that takes \\(M\\) \\(p\\)-values as an argument and returns a \\(p\\)-value that behaves like a univariate \\(p\\)-value for a test of \\(H_0\\). There are two kinds both based on the null distributions of \\(\\mathbf{p} = p_1, \\dots, p_M\\). The first kind uses \\(p_{(k)}\\), the \\(k\\)th order statistic of \\(\\mathbf{p}\\). Under \\(H_0\\), the probability that \\(p_{(k)}\\) is less than or equal to \\(q\\) is the probability that \\(k\\) or more uniform \\(p\\)-values less than \\(q\\). In a formula, that is \\[F_{(k)}(q) = P(p_{(k)} \\leq q) = \\sum_{l = k}^M q^l (1 - q)^{M - l}.\\] Therefore, \\(F_{(k)}(p_{(k)}) \\sim U\\) and we have our first pooled \\(p\\)-value \\(g_{(k)}(\\mathbf{p}) = F_{(k)}(p_{(k)})\\). One example of this is from Tippett (1931), which uses \\(p_{(1)}\\) to obtain \\[g_{Tip}(\\mathbf{p}) = 1 - (1 - p_{(1)})^M,\\] the exact version of the commonly used approximation \\(Mp_{(1)}\\) known as the Bonferroni correction. The second kind of pooled \\(p\\)-value makes use of quantile transformations. For a continuous random variable \\(X\\) with cumulative distribution function (CDF) \\(F\\) and quantile function \\(F^{-1}\\), \\(F^{-1}(U) = X\\). If \\(X\\) is known to have a CDF \\(F_M\\) under summation, the quantities \\[g(\\mathbf{p}) = 1 - F_M \\left ( \\sum_{i = 1}^M F^{-1}(1 - p_i) \\right )\\] and \\[g(\\mathbf{p}) = F_M \\left ( \\sum_{i = 1}^M F^{-1}(p_i) \\right )\\] will both behave exactly like a univariate \\(p\\)-value. Obvious choices for \\(X\\) are the normal distribution and gamma distribution because both of these distributions are closed under summation. The majority of proposed pooled \\(p\\)-values follow the first of these quantile transformation methods, including \\(g_{Fis}\\) of Fisher (1932) (using the \\(\\chi^2_2\\) distribution) and \\(g_{Sto}\\) of Stouffer et al. (1949) (using the \\(N(0,1)\\) distribution). Let’s code all of these up: ## the order statistic function gord &lt;- function(p, k) { pk &lt;- sort(p, decreasing = FALSE)[k] # kth order statistic 1 - pbinom(k - 1, length(p), pk) } ## the quantile transformation function gquant &lt;- function(p, finv, fm) { 1 - fm(sum(finv(1 - p))) } ## make particular instances gtip &lt;- function(p) gord(p, 1) gfis &lt;- function(p) gquant(p, finv = function(p) qchisq(p, 2), fm = function(s) pchisq(s, 2 * length(p))) gsto &lt;- function(p) gquant(p, finv = qnorm, fm = function(s) pnorm(s, sd = sqrt(length(p)))) ## store together in a list pools &lt;- list(tippett = gtip, fisher = gfis, stouffer = gsto) We can now compute the pooled \\(p\\)-values of our earlier Mendelian and non-Mendelian cases: (poolMendel &lt;- lapply(pools, function(f) f(pMendel))) ## $tippett ## [1] 0 ## ## $fisher ## [1] 0 ## ## $stouffer ## [1] 0 (poolNonMend &lt;- lapply(pools, function(f) f(pnonMendel))) ## $tippett ## [1] 0.5739556 ## ## $fisher ## [1] 0.01703636 ## ## $stouffer ## [1] 0.005196858 All methods agree for the first set of \\(p\\)-values (not too surprising given that the first \\(p\\)-value is less than \\(1 \\times 10^{-27}\\)), but the non-Mendelian case leads to some interesting differences. Let’s look at the two sets of \\(p\\)-values again: rbind(Mendel = round(pMendel, 2), `Non-Mendel` = round(pnonMendel, 2)) ## D1Mit475 D2Mit312 D3Mit60 D4Mit149 D5Mit331 D6Mit86 D7Mit21 Fcer2a ## Mendel 0.00 0.56 0.57 0.72 0.77 0.31 0.51 0.54 ## Non-Mendel 0.85 0.14 0.09 0.19 0.08 0.41 0.70 0.05 ## D9Mit186 D10Mit166 D11Mit1 D12Mit37 D13Mit158 D14Mit179 D15Mit12 ## Mendel 0.67 0.41 0.04 0.29 0.55 0.21 0.40 ## Non-Mendel 0.15 0.15 0.04 0.16 0.42 0.57 0.82 ## D16Mit32 Tfb1m D18Mit66 D19Mit32 DXMit26 ## Mendel 0.14 0.10 0.57 0.77 0.86 ## Non-Mendel 0.52 0.11 0.19 0.09 0.82 When only one marker is responsible for the variation in the trait, a single \\(p\\)-value is highly significant. In the case of many responsible markers the \\(p\\)-values are not so obviously significant, with the smallest one barely less than 0.05. Rather, this case has a greater number of small \\(p\\)-values than we would expect by chance if all are truly uniform and independent, a fact more easily seen by viewing a histogram of the two sets of \\(p\\)-values. hist(pMendel, breaks = seq(0, 1, by = 0.2), ylim = c(0, 12)) hist(pnonMendel, breaks = seq(0, 1, by = 0.2), ylim = c(0, 12)) Though the proof is beyond the scope of this workshop, it turns out the ordering of \\(g_{Tip}\\), \\(g_{Fis}\\), and \\(g_{Sto}\\) for the non-Mendelian case reflects a deeper concept. Broadly, any pooled \\(p\\)-value is sensitive to either a large number of weakly significant \\(p\\)-values or a small number of strongly significant \\(p\\)-values. There is a fundamental trade-off between these two cases for any pooled \\(p\\)-value, which we can visualize crudely using the mean and minimum values of each simulated null sample in pNull. The following plot shows the pooled \\(p\\)-value for each method by the mean of the sample, meant to represent the evidence spread broadly. Point sizes for each mean value are scaled by the minimum \\(p\\)-value, meant to represent concentrated evidence in one test. ## compute medians for each sample pNullMean &lt;- apply(t(pNull), 1, mean) ## and the minimum pNullMin &lt;- apply(t(pNull), 1, min) ## as well as each pooled p-value pNullPools &lt;- apply(t(pNull), 1, function(row) sapply(pools, function(f) f(row))) ## plot the pooled p-value by mean of sample plot(pNullMean, pNullPools[&quot;fisher&quot;, ], pch = 16, ylab = &quot;Pooled p-value&quot;, cex = pNullMin * 4 + 0.3, xlab = &quot;Mean of p-value sample&quot;, col = adjustcolor(&quot;firebrick&quot;, 0.3)) points(pNullMean, pNullPools[&quot;stouffer&quot;, ], pch = 16, cex = pNullMin * 4 + 0.3, col = adjustcolor(&quot;steelblue&quot;, 0.3)) points(pNullMean, pNullPools[&quot;tippett&quot;, ], pch = 16, cex = pNullMin * 4 + 0.3, col = adjustcolor(&quot;seagreen&quot;, 0.3)) legend(x = &quot;topleft&quot;, col = adjustcolor(c(&quot;firebrick&quot;, &quot;steelblue&quot;, &quot;seagreen&quot;), 0.3), legend = c(expression(g[Fis]), expression(g[Sto]), expression(g[Tip])), pch = 16) Note how \\(g_{Sto}\\) is increasing almost monotonically in the mean, with large and small points spread evenly throughout the line for each mean value. \\(g_{Fis}\\), as suggested by its position giving a \\(p\\)-value between \\(g_{Sto}\\) and \\(g_{Tip}\\), follows this line somewhat, but is somewhat sensitive to the minimum. The large points, indicating samples with large minimum values, tend to have a larger \\(p\\)-value than the \\(g_{Sto}\\) line while the small points, samples with small minimum values, tend to have a smaller \\(p\\)-value. \\(g_{Tip}\\) barely follows the line at all, the only relationship we can see in the green points is between the size of the point and its \\(y\\)-value. All of this to say: \\(g_{Sto}\\) is not sensitive to the minimum, it follows the overall evidence spread among the \\(p\\)-values \\(g_{Fis}\\) balances sensitivity to the minimum and overall spread of \\(p\\)-values \\(g_{Tip}\\) is not sensitive to the overall spread of evidence, it only responds to the minimum Of course, these examples all assume independence of the \\(p\\)-values. In the case of dependence, the idea of what “spread” evidence means is a lot less clear. Are multiple small \\(p\\)-values an indication of multiple independent contributing variables, or simply an artifact of correlations between them? 8.3 Adjusting for dependence A comprehensive overview of the methods used to adjust for dependence can be found in the companion paper to the poolr package, Cinar and Viechtbauer (2022). It should come as no surprise that the above functions have been implemented in this package alongside the adjustments. We’ll take a brief aside to familiarize ourselves with them. Instead of using the functions we defined previously, we can take library(poolr) ## the naming is more convenient than ours (pfMend &lt;- fisher(pMendel, adjust = &quot;none&quot;)) ## combined p-values with: Fisher&#39;s method ## number of p-values combined: 20 ## test statistic: 161.118 ~ chi-square(df = 40) ## adjustment: none ## combined p-value: 1.817574e-16 (psMend &lt;- stouffer(pMendel, adjust = &quot;none&quot;)) # more accurate for small p-values ## combined p-values with: Stouffer&#39;s method ## number of p-values combined: 20 ## test statistic: 2.894 ~ N(0,1) ## adjustment: none ## combined p-value: 0.00190034 (ptMend &lt;- tippett(pMendel, adjust = &quot;none&quot;)) ## combined p-values with: Tippett&#39;s method ## number of p-values combined: 20 ## minimum p-value: 0 ## adjustment: none ## combined p-value: 0 ## non-Mendelian case (pfNM &lt;- fisher(pnonMendel, adjust = &quot;none&quot;)) ## combined p-values with: Fisher&#39;s method ## number of p-values combined: 20 ## test statistic: 61.208 ~ chi-square(df = 40) ## adjustment: none ## combined p-value: 0.01703636 (psNM &lt;- stouffer(pnonMendel, adjust = &quot;none&quot;)) ## combined p-values with: Stouffer&#39;s method ## number of p-values combined: 20 ## test statistic: 2.562 ~ N(0,1) ## adjustment: none ## combined p-value: 0.005196858 (ptNM &lt;- tippett(pnonMendel, adjust = &quot;none&quot;)) ## combined p-values with: Tippett&#39;s method ## number of p-values combined: 20 ## minimum p-value: 0.042 ## adjustment: none ## combined p-value: 0.5739556 We might immediately notice that this is more accurate for the small \\(p\\)-value occurring in the Mendelian example and has some slightly different values for the non-Mendelian example. This is a direct result of the way gquant is defined, we can achieve the same result as poolr by using the lower.tail argument rather than subtracting values from 1. Computational statistics is not quite the same as mathematical statistics (making all of our lives harder). ## our function gave a zero for gsto and the mendelian data, can we ## recreate poolr&#39;s 0.0019? pnorm(sum(qnorm(pMendel, lower.tail = FALSE))/sqrt(20), lower.tail = FALSE) ## [1] 0.00190034 In any case, the ordering with these more accurate \\(p\\)-values mirrors the non-Mendelian case. When evidence is concentrated in one test \\(g_{Tip} &lt; g_{Fis} &lt; g_{Sto}\\) while when evidence is spread \\(g_{Tip} &gt; g_{Fis} &gt; g_{Sto}\\). With that very brief exploration out of the way, we can define some correlated data using our filtered panel from before. Let’s take a random sample of markers on chromosome 1. set.seed(124093) inds &lt;- sort(sample(which(mgiFiltered$markers$chr == &quot;1&quot;), 20)) mgiDep &lt;- mgiFiltered$data[, inds] # select sampled inds mgiDepCors &lt;- cor(mgiDep) # important for later ## image correlations between these image(mgiDepCors) 8.3.1 The wrong way If we check the first few lines of code for any of the poolr pooling functions, take fisher, we’ll see several options for adjustment: head(fisher, 7) ## ## 1 function (p, adjust = &quot;none&quot;, R, m, size = 10000, threshold, ## 2 side = 2, batchsize, nearpd = TRUE, ...) ## 3 { ## 4 p &lt;- .check.p(p) ## 5 k &lt;- length(p) ## 6 adjust &lt;- match.arg(adjust, c(&quot;none&quot;, &quot;nyholt&quot;, &quot;liji&quot;, &quot;gao&quot;, ## 7 &quot;galwey&quot;, &quot;empirical&quot;, &quot;generalized&quot;)) The first four of these after “none” are all based on the concept of effective number of tests. Basically, these compute the eigenvalues of the correlation matrix between the tests used to generate \\(\\mathbf{p}\\) and use some function of these eigenvalues to estimate an effective number of independent tests. Much like in principal components analysis (PCA), this assumes that most of the information in \\(M\\) \\(p\\)-values can be summarized by a smaller number of tests, \\(m\\). Unlike PCA, however, these methods do not select the important tests, rather they just take the entirely ad-hoc \\[g_{adj}(\\mathbf{p}) = 1 - F_M \\left ( \\frac{m}{M} \\sum_{i = 1}^M F^{-1}(1 - p_i) \\right )\\] instead of \\[g_{adj}(\\mathbf{p}) = 1 - F_M \\left ( \\sum_{i = 1}^M F^{-1}(1 - p_i) \\right ).\\] Let’s explore the result. ## start by generating traits as before set.seed(2314) depMendel &lt;- mgiDep[, 1] + rnorm(94, sd = 0.3) plot(density(depMendel)) depNonMend &lt;- apply(mgiDep[, 1:20], 1, mean) + rnorm(94, sd = 0.3) plot(density(depNonMend)) ## these look basically the same as before, despite the correlations ## what about the p-values? (pMendelDep &lt;- apply(mgiDep, 2, function(col) cor.test(depMendel, col)$p.value)) ## D1Mit475 D1Mit412 D1Mit245 D1Mit248 D1Mit214 D1Mit303 ## 8.140813e-28 1.429891e-08 1.429891e-08 1.340364e-06 3.717356e-03 6.113978e-03 ## D1Mit480 D1Mit325 D1Mit180 D1Mit81 D1Mit305 D1Mit188 ## 6.113978e-03 2.076697e-02 9.091619e-01 4.944939e-01 3.151411e-01 1.821606e-01 ## D1Mit218 Cxcr4 D1Mit445 D1Mit346 Fmn2 D1Mit404 ## 1.681507e-01 4.848012e-01 2.527444e-01 3.308191e-01 3.921758e-01 6.029271e-01 ## D1Mit273 D1Mit37 ## 4.992006e-01 5.305738e-01 ## based on the non-mendelian trait (pNonMenDep &lt;- apply(mgiDep, 2, function(col) cor.test(depNonMend, col)$p.value)) ## D1Mit475 D1Mit412 D1Mit245 D1Mit248 D1Mit214 D1Mit303 ## 7.098574e-02 4.859396e-05 4.859396e-05 3.599978e-05 1.312435e-07 1.308804e-07 ## D1Mit480 D1Mit325 D1Mit180 D1Mit81 D1Mit305 D1Mit188 ## 1.308804e-07 3.882606e-08 1.643775e-10 4.334330e-10 6.486238e-10 4.387799e-10 ## D1Mit218 Cxcr4 D1Mit445 D1Mit346 Fmn2 D1Mit404 ## 1.551436e-09 1.711769e-08 4.041306e-11 6.675577e-10 3.787253e-06 1.401357e-06 ## D1Mit273 D1Mit37 ## 8.044158e-06 4.581290e-05 Dependence between the tests pulls the \\(p\\)-values down universally. While in the previous case only a handful of tests would lead to rejection at the canonical \\(\\alpha = 0.05\\), almost every test is rejected in both cases here. If we did not account for this known correlation, our pooled \\(p\\)-values will be universally smaller as a result and we will reject \\(H_0\\) far more often than we should. Do the adjustments make a difference? ## trying all effective test adjustments on both data sets adjustments &lt;- c(none = &quot;none&quot;, nyholt = &quot;nyholt&quot;, liji = &quot;liji&quot;, gao = &quot;gao&quot;, galwey = &quot;galwey&quot;) ## having to suppress warnings is always a good sign... suppressWarnings(adjMenFis &lt;- sapply(adjustments, function(a) fisher(p = pMendelDep, adjust = a, R = mgiDepCors))) suppressWarnings(adjMenTip &lt;- sapply(adjustments, function(a) tippett(p = pMendelDep, adjust = a, R = mgiDepCors))) suppressWarnings(adjMenSto &lt;- sapply(adjustments, function(a) stouffer(p = pMendelDep, adjust = a, R = mgiDepCors))) ## compare p-values between methods rbind(Tippett = adjMenTip[&quot;p&quot;, ], Fisher = adjMenFis[&quot;p&quot;, ], Stouffer = adjMenSto[&quot;p&quot;, ]) ## none nyholt liji gao galwey ## Tippett 0 0 0 0 0 ## Fisher 6.087399e-39 5.897589e-28 4.394485e-15 5.897589e-28 4.394485e-15 ## Stouffer 3.720353e-18 2.969156e-13 1.766807e-07 2.969156e-13 1.766807e-07 ## and &#39;effective number of tests&#39; rbind(Tippett = adjMenTip[&quot;m&quot;, ], Fisher = adjMenFis[&quot;m&quot;, ], Stouffer = adjMenSto[&quot;m&quot;, ]) ## none nyholt liji gao galwey ## Tippett NULL 14 7 14 7 ## Fisher NULL 14 7 14 7 ## Stouffer NULL 14 7 14 7 These adjustments definitely help reduce our problem, every single \\(p\\)-value increased considerably, but they disagree considerably on the effective number of tests. It’s worth noting that the effective number of tests here has nothing to do with our Mendelian generation of traits, it looks at the correlations alone. This makes its interpretation difficult, to say the least. Perhaps the non-Mendelian case will fare better. suppressWarnings(adjNMFis &lt;- sapply(adjustments, function(a) fisher(p = pNonMenDep, adjust = a, R = mgiDepCors))) suppressWarnings(adjNMTip &lt;- sapply(adjustments, function(a) tippett(p = pNonMenDep, adjust = a, R = mgiDepCors))) suppressWarnings(adjNMSto &lt;- sapply(adjustments, function(a) stouffer(p = pNonMenDep, adjust = a, R = mgiDepCors))) rbind(Tippett = adjNMTip[&quot;p&quot;, ], Fisher = adjNMFis[&quot;p&quot;, ], Stouffer = adjNMSto[&quot;p&quot;, ]) ## none nyholt liji gao galwey ## Tippett 8.082601e-10 5.657821e-10 2.82891e-10 5.657821e-10 2.82891e-10 ## Fisher 3.856394e-107 8.31352e-76 3.4097e-39 8.31352e-76 3.4097e-39 ## Stouffer 3.515608e-111 1.715434e-78 2.692654e-40 1.715434e-78 2.692654e-40 rbind(Tippett = adjNMTip[&quot;m&quot;, ], Fisher = adjNMFis[&quot;m&quot;, ], Stouffer = adjNMSto[&quot;m&quot;, ]) ## none nyholt liji gao galwey ## Tippett NULL 14 7 14 7 ## Fisher NULL 14 7 14 7 ## Stouffer NULL 14 7 14 7 As the correlation structure is identical, the effective number of tests have not changed at all. However, the much smaller \\(p\\)-values across the board in the non-Mendelian case lead to much smaller pooled \\(p\\)-values, even with adjustment. Using the correlation between markers as a proxy for correlation between tests is somewhat unfair (though it is exactly what the original proposals suggest), but even under more favourable assumptions these adjustments fail. Consider, for example, the case where \\(l\\) of the \\(M\\) \\(p\\)-values are identical, or nearly so. If we denote the independent tests with \\(x_1, \\dots, x_{k - l}\\) and the dependent tests with \\(y_1, \\dots, y_l\\), we can write \\[\\tilde{X}^2 = \\frac{m}{M} X^2 = \\frac{m}{M} \\left ( \\sum_{i = 1}^{M-l} x_i + \\sum_{j = 1}^l y_j \\right ).\\] Supposing that we correctly identify that \\(m = M - l + 1\\) and letting \\(y^*\\) represent the single unique value taken by \\(y_1, \\dots, y_l\\) gives \\[\\frac{M - l + 1}{M} \\left ( \\sum_{i=1}^{M-l} x_i + l y^* \\right ).\\] Now, if \\(x_i \\sim \\chi^2_{(2)}\\) and \\(y^* \\sim \\chi^2_{(2)}\\) all independently and identically (We’re using \\(g_{Fis}\\)), this sum is not generally \\(\\chi^2\\) distributed. The same is true for the rescaled statistic of Stouffer’s method. Why these theoretically invalid methods with unclear interpretations were included in poolr is unclear, but they probably shouldn’t be used. 8.3.2 A better way Instead, we’re better off relying on good old-fashioned simulation and normal approximations, as these are both at least asymptotically valid. The \"empirical\" and \"generalized\" methods offer these two options, respectively. ## the valid adjustment methods adjustments &lt;- c(adjustments, empirical = &quot;empirical&quot;, generalized = &quot;generalized&quot;) ## compare the resulting p-values suppressWarnings(adjMenFis &lt;- sapply(adjustments, function(a) fisher(p = pMendelDep, adjust = a, R = mgiDepCors))) suppressWarnings(adjMenTip &lt;- sapply(adjustments[-length(adjustments)], function(a) tippett(p = pMendelDep, adjust = a, R = mgiDepCors))) suppressWarnings(adjMenSto &lt;- sapply(adjustments, function(a) stouffer(p = pMendelDep, adjust = a, R = mgiDepCors))) rbind(Fisher = adjMenFis[&quot;p&quot;, ], Tippett = c(adjMenTip[&quot;p&quot;, ], NA), Stouffer = c(adjMenSto[&quot;p&quot;, ])) ## none nyholt liji gao galwey ## Fisher 6.087399e-39 5.897589e-28 4.394485e-15 5.897589e-28 4.394485e-15 ## Tippett 0 0 0 0 0 ## Stouffer 3.720353e-18 2.969156e-13 1.766807e-07 2.969156e-13 1.766807e-07 ## empirical generalized ## Fisher 9.999e-05 6.338611e-19 ## Tippett 9.999e-05 NA ## Stouffer 0.00089991 0.001863063 For the Mendelian case, we can see the highly inflated significance is reduced more by the empirical and generalized methods, which produce \\(p\\)-values similar to the independent case. This is a bit deceptive for the empirical version, as it is based on simulation and so can’t report \\(p\\)-values less than \\(1/n_r\\) where \\(n_r\\) is the number of repetitions. For the generalized method, at least, that the \\(p\\)-value for Stouffer is closer to the independent case suggests this combination is accounting for dependence more successfully. As the generation method is identical for the independent and dependent case, we might expect close \\(p\\)-values. ## compare the resulting p-values suppressWarnings(adjNMFis &lt;- sapply(adjustments, function(a) fisher(p = pNonMenDep, adjust = a, R = mgiDepCors))) suppressWarnings(adjNMTip &lt;- sapply(adjustments[-length(adjustments)], function(a) tippett(p = pNonMenDep, adjust = a, R = mgiDepCors))) suppressWarnings(adjNMSto &lt;- sapply(adjustments, function(a) stouffer(p = pNonMenDep, adjust = a, R = mgiDepCors))) rbind(Fisher = adjNMFis[&quot;p&quot;, ], Tippett = c(adjNMTip[&quot;p&quot;, ], NA), Stouffer = c(adjNMSto[&quot;p&quot;, ])) ## none nyholt liji gao galwey ## Fisher 3.856394e-107 8.31352e-76 3.4097e-39 8.31352e-76 3.4097e-39 ## Tippett 8.082601e-10 5.657821e-10 2.82891e-10 5.657821e-10 2.82891e-10 ## Stouffer 3.515608e-111 1.715434e-78 2.692654e-40 1.715434e-78 2.692654e-40 ## empirical generalized ## Fisher 9.999e-05 4.233807e-50 ## Tippett 9.999e-05 NA ## Stouffer 9.999e-05 2.36726e-14 Of course, the drawback of the empirical method in particular is that is does not capture the tail behaviour of these methods well. The same \\(p\\)-value is repeated for all methods across both the Mendelian and non-Mendelian cases based purely on the number of simulated repetitions performed. Unfortunately, increasing the number of repetitions defeats the purpose of pooled \\(p\\)-values, as it increases the computational burden. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
