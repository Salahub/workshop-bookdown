[["pooled-p-values.html", "Chapter 8 Pooled p-values 8.1 A motivating example 8.2 Pooling \\(p\\)-values 8.3 Adjusting for dependence", " Chapter 8 Pooled p-values Chris Salahub 8.1 A motivating example Let’s say we have collected a bunch of genetic data and would like to identify which genes are related to some observed trait. We’ll start by defining some terms to make describing things easier. DNA: genetic information that is passed between generations and is stored in several big molecules made up of many small molecules called nucleotides genome: all the DNA of an organism gene: a segment of DNA that gives the blueprint for a particular protein chromosomes: one of the several big molecules making up the genome marker: a sequence of nucleotides measured at a known position in DNA sex: the creation of an organism in a new generation by recombining DNA from two organisms in the previous generation recombination: when DNA is changed slightly by segments of DNA literally crossing each other and switching or chromosomes being inherited independently, pictorially: centiMorgans: an additive measure of distance on DNA, for two markers which have probability \\(q\\) of being separated during sex the distance in centiMorgans is \\(d = 50 \\ln \\frac{1}{1 - 2q}\\) So, when we talk about genetic data, we mean markers at known positions on chromosomes which ideally span the whole genome of a sample of organisms. We pair this data with a physical measurement - in humans examples include height, eye colour, hair colour, or the presence of a disease - to determine how much physical variation can be explained by genetic variation. Physical traits that are linked to genetics are either Mendelian or non-Mendelian (you can guess which came first). Mendelian traits have variation explained by one (or very few) gene(s), take eye colour which is mostly down to the gene OCA2. Non-Mendelian traits, like height, have variation which cannot be attributed to one (or a handful of) genetic region(s) alone. To identify which measured markers are related to a physical trait, a host of significance tests are typically computed to assay the impact of each. While this is not optimal compared to a linear model, it is frequently the only option for genetic studies which collect hundred of thousands of markers for only a few dozen individuals. Difficulty fitting a model practically means we must filter these markers first. Before even pointing our finger at a subset of genes, however, we might want to save time by asking whether there is anything interesting happening at all in this collection of tests. That is where pooled \\(p\\)-values come in. 8.1.1 Some “real” data Our ideal data set would involve humans. Genomics mostly strives to better understand the genetic sources of human diseases and traits to help create better medicine and understand ourselves more fully. Unfortunately, human data is pretty hard to come by ethically, and mice data is a lot easier to find. The code below pulls some that is freely available from Mouse Genome Informatics. ## this script provides a means of downloading mouse genomic data from the ## mouse genome database (MGD), an online repository of data on mouse ## genetics ## all the data is stored in text files at mgiUrl &lt;- &quot;https://www.informatics.jax.org/downloads/reports/&quot; ## we&#39;ll be looking at data from mgiPName &lt;- &quot;MGI_JAX_BSB_Panel.rpt&quot; ## and for reference we&#39;ll also need the marker lists in mgiMNames &lt;- c(&quot;MRK_List1.rpt&quot;, &quot;MRK_List2.rpt&quot;) ## this function reads in MGI text files, which are basically organized ## as a tab-separated data frame with extra information readMGIrpt &lt;- function(file) { raw &lt;- scan(file, what = character(), sep = &quot;\\n&quot;) # most basic R input leg &lt;- which(raw == &quot;Legend:&quot;) # identify the legend rows lenHead &lt;- leg + 4 # the +4 is from inspection of the legends if (length(leg) == 0) { # separate legend leg &lt;- which(grepl(&quot;^CHR&quot;, raw))[1] lenHead &lt;- leg } desc &lt;- paste(raw[1:lenHead], collapse = &quot;\\n&quot;) # data description dat &lt;- raw[(lenHead+1):length(raw)] # actual data ## each row in the data is associated with a reference, the study ## or paper that characterized or collected the corresponding ## observation, we want to separate these refPos &lt;- regexec(&quot;\\\\tJ\\\\:[0-9]+(?:, J\\\\:[0-9]+){0,4}&quot;, dat) refs &lt;- sapply(regmatches(dat, refPos), # extract references function(el) { if (length(el) == 0) { &quot;&quot; } else gsub(&quot;\\\\t&quot;, &quot;&quot;, el)}) data &lt;- unlist(regmatches(dat, refPos, invert = TRUE)) # remove refs mat &lt;- do.call(rbind, strsplit(data[data != &quot;&quot;], &quot;\\\\t&quot;)) # regularize rwnms &lt;- mat[1, -(1:3)] # animal numbers/ids colnms &lt;- mat[-1, 3] # symbol field colDesc &lt;- mat[-1, 1:3] # symbol details colnames(colDesc) &lt;- c(&quot;chr&quot;, &quot;mgiid&quot;, &quot;symbol&quot;) # informative names data &lt;- t(mat[-1,-(1:3)]) # observed values rownames(data) &lt;- rwnms # final data formatting colnames(data) &lt;- colnms list(summary = desc, # return everything in a big list markers = data.frame(colDesc, ref = refs[-1]), data = as.data.frame(data)) } ## this function looks at the marker reference data and processes it into ## a form easier for future analysis readMGIlists &lt;- function(fileList = paste0(mgiUrl, mgiMNames)) { lists &lt;- lapply(fileList, scan, # similar input to above what = character(), sep = &quot;\\n&quot;) lists &lt;- lapply(lists, gsub, # replace end of line tabs with periods pattern = &quot;\\t$&quot;, replacement= &quot;\\t.&quot;) splits &lt;- lapply(lists, strsplit, split = &quot;\\t&quot;) # split by tabs colnms &lt;- splits[[1]][[1]] # column names data &lt;- do.call(rbind, # covert into one big matrix lapply(splits, function(splt) do.call(rbind, splt[-1]))) colnames(data) &lt;- colnms as.data.frame(data) } ## using indices and a reference table, process centiMorgan positions processcMs &lt;- function(inds, tab) { sel &lt;- tab[inds] # take indices from table sel[grepl(&quot;syntenic&quot;, sel)] &lt;- &quot;Inf&quot; suppressWarnings(as.numeric(sel)) # warnings by design } ## this function goes through a processed panel from the above functions ## and drops the measurements that have un filterPanelBycM &lt;- function(panel, locs) { locOrd &lt;- order(locs) # order position toKeep &lt;- is.finite(locs[locOrd]) # drop NAs and Infs outMrk &lt;- data.frame(panel$markers[locOrd[toKeep],], cMs = locs[locOrd[toKeep]]) outMrk &lt;- outMrk[order(outMrk$chr),] # order chromosome list(summary = panel$summary, markers = outMrk, data = panel$data[, outMrk$symbol]) } ## pull the marker reference mgiMarkers &lt;- readMGIlists() # descriptions of all markers ## pull the panel mgiPanel &lt;- readMGIrpt(paste0(mgiUrl, mgiPName)) ## match the marker names back to the marker reference mgiPanel.mrkr &lt;- match(names(mgiPanel$data), mgiMarkers$`Marker Symbol`) ## get cM positions mgiPanel.cMs &lt;- processcMs(mgiPanel.mrkr, tab = mgiMarkers$`cM Position`) ## Infs indicate markers localized to a chromosome but without a ## known cM position, while NAs indicate markers missing from the ## reference file or marked as unknown there ## filter the panels by markers with known positions mgiFiltered &lt;- filterPanelBycM(mgiPanel, mgiPanel.cMs) ## and convert the data to numeric values mgiFiltered$data &lt;- as.data.frame(lapply(mgiFiltered$data, function(mrkr) as.numeric(mrkr== &quot;b&quot;))) ## remove intermediate data rm(list = c(&quot;mgiMarkers&quot;, &quot;mgiPanel.cMs&quot;, &quot;mgiPanel.mrkr&quot;, &quot;mgiPanel&quot;)) summary(mgiFiltered) ## Length Class Mode ## summary 1 -none- character ## markers 5 data.frame list ## data 1598 data.frame list dim(mgiFiltered$data) ## [1] 94 1598 In the end, after reading and cleaning the data, we have 94 mice each measured at 1598 marker locations. We are still in the unfortunate position of lacking physical measurements for any of the 94 mice on this panel, but can still generate pseudo \\(p\\)-values. For Mendelian traits, we can pick a marker and generate a trait based solely on its value, while non-Mendelian traits will require the selection of numerous locations which may not contribute equally. For now, let’s keep this as simple as possible by ensuring independence. Chromosomes assort independently, so if we take one marker from each chromosome we should have independent markers. ## match in this context will give the index of the first marker on ## each chromosome mgiIndep &lt;- mgiFiltered$data[, match(c(1:19, &quot;X&quot;), mgiFiltered$markers$chr)] ## inspect dependence structure image(cor(mgiIndep)) To simulate a Mendelian trait, we’ll simply add noise to the first marker. For non-Mendelian inheritance, let’s just take the average of all markers and add noise. set.seed(2314) traitMendel &lt;- mgiIndep[, 1] + rnorm(94, sd = 0.3) plot(density(traitMendel)) # what mendelian traits look like traitNonMend &lt;- apply(mgiIndep[, 1:20], 1, mean) + rnorm(94, sd = 0.3) plot(density(traitNonMend)) # what non-mendelian traits look like Finally, we can compute \\(p\\)-values for each marker and the traits based on the correlation between them. ## based on the mendelian trait (pMendel &lt;- apply(mgiIndep, 2, function(col) cor.test(traitMendel, col)$p.value)) ## D1Mit475 D2Mit312 D3Mit60 D4Mit149 D5Mit331 D6Mit86 ## 8.140813e-28 5.562233e-01 5.701328e-01 7.242746e-01 7.675526e-01 3.105180e-01 ## D7Mit21 Fcer2a D9Mit186 D10Mit166 D11Mit1 D12Mit37 ## 5.065030e-01 5.402953e-01 6.714437e-01 4.083773e-01 4.271063e-02 2.917104e-01 ## D13Mit158 D14Mit179 D15Mit12 D16Mit32 Tfb1m D18Mit66 ## 5.489663e-01 2.092645e-01 4.023976e-01 1.364540e-01 1.034232e-01 5.719812e-01 ## D19Mit32 DXMit26 ## 7.726304e-01 8.591501e-01 ## based on the non-mendelian trait (pnonMendel &lt;- apply(mgiIndep, 2, function(col) cor.test(traitNonMend, col)$p.value)) ## D1Mit475 D2Mit312 D3Mit60 D4Mit149 D5Mit331 D6Mit86 D7Mit21 ## 0.84662803 0.13598050 0.09371634 0.19260532 0.07979980 0.40937430 0.69903954 ## Fcer2a D9Mit186 D10Mit166 D11Mit1 D12Mit37 D13Mit158 D14Mit179 ## 0.04886178 0.14942358 0.14839598 0.04176343 0.15557356 0.42105667 0.57307268 ## D15Mit12 D16Mit32 Tfb1m D18Mit66 D19Mit32 DXMit26 ## 0.82419580 0.51590067 0.10878820 0.18622075 0.09006944 0.81802465 ## some null cases pNull &lt;- replicate(1000, apply(mgiIndep, 2, function(col) cor.test(runif(94), col)$p.value)) head(t(pNull)) ## D1Mit475 D2Mit312 D3Mit60 D4Mit149 D5Mit331 D6Mit86 D7Mit21 ## [1,] 0.90978137 0.9069532 0.35006885 0.52959032 0.7380976 0.7379150 0.7759258 ## [2,] 0.71128190 0.4672418 0.40962358 0.55101467 0.2559132 0.9183818 0.3258728 ## [3,] 0.95942070 0.5427160 0.04764853 0.06797646 0.7925067 0.4583663 0.8189367 ## [4,] 0.72433189 0.2469022 0.37670242 0.79942407 0.7073759 0.4569672 0.5144509 ## [5,] 0.09880001 0.5690093 0.04219049 0.67880376 0.5211135 0.2129442 0.5705933 ## [6,] 0.47118092 0.1661241 0.87974774 0.85674033 0.9537631 0.7554583 0.9164587 ## Fcer2a D9Mit186 D10Mit166 D11Mit1 D12Mit37 D13Mit158 D14Mit179 ## [1,] 0.6541013 0.892077786 0.0941390 0.9816285 0.58747481 0.96585574 0.10030964 ## [2,] 0.7740573 0.928080700 0.5653199 0.9420153 0.60334296 0.44579138 0.02940274 ## [3,] 0.2356859 0.830787629 0.7194775 0.3474443 0.72685240 0.95146071 0.68194085 ## [4,] 0.2079808 0.008370045 0.1754181 0.6098431 0.03384509 0.85023276 0.01777558 ## [5,] 0.2412231 0.034974937 0.8137640 0.8319588 0.64930667 0.06533281 0.23257463 ## [6,] 0.6922345 0.695358501 0.9634527 0.1120413 0.93632232 0.82351369 0.35253150 ## D15Mit12 D16Mit32 Tfb1m D18Mit66 D19Mit32 DXMit26 ## [1,] 0.9908900 0.3714854 0.5952756 0.76844887 0.6071990 0.72313619 ## [2,] 0.7466969 0.9263798 0.8664269 0.61143617 0.3156817 0.06558521 ## [3,] 0.7392575 0.6634759 0.7311247 0.68270873 0.3402068 0.45887828 ## [4,] 0.7151886 0.6484924 0.6415287 0.68611425 0.8145712 0.11268825 ## [5,] 0.9612551 0.1987970 0.8847019 0.93284250 0.5656709 0.55879076 ## [6,] 0.9654213 0.8425012 0.7445170 0.06411416 0.3992616 0.49837159 8.2 Pooling \\(p\\)-values Whether inheritance is Mendelian, non-Mendelian, or something else, our selection of markers tested against a trait gives a collection of \\(M=20\\) \\(p\\)-values from 20 hypothesis tests, and we’d like to know if further investigation is warranted for any of them. Directly, we want to test the null hypothesis \\[H_0 = \\cap_{i = 1}^M H_{0i}\\] where \\(H_{0i}\\) is the hypothesis of “no association” the \\(i\\)th marker. Another way of thinking about this is by considering the null distributions of our \\(M\\) \\(p\\)-values \\(p_1, \\dots, p_M\\) if there is nothing going on. In this case, we’d expect \\[H_0 = p_1, p_2, \\dots, p_M \\overset{\\mathrm{iid}}{\\sim} U\\] where \\(U = Unif(0,1)\\). The basic idea of pooled \\(p\\)-values is to think of a function that takes \\(M\\) \\(p\\)-values as an argument and returns a \\(p\\)-value that behaves like a univariate \\(p\\)-value for a test of \\(H_0\\). There are two kinds both based on the null distributions of \\(\\mathbf{p} = p_1, \\dots, p_M\\). The first kind uses \\(p_{(k)}\\), the \\(k\\)th order statistic of \\(\\mathbf{p}\\). Under \\(H_0\\), the probability that \\(p_{(k)}\\) is less than or equal to \\(q\\) is the probability that \\(k\\) or more uniform \\(p\\)-values less than \\(q\\). In a formula, that is \\[F_{(k)}(q) = P(p_{(k)} \\leq q) = \\sum_{l = k}^M q^l (1 - q)^{M - l}.\\] Therefore, \\(F_{(k)}(p_{(k)}) \\sim U\\) and we have our first pooled \\(p\\)-value \\(g_{(k)}(\\mathbf{p}) = F_{(k)}(p_{(k)})\\). One example of this is from Tippett (1931), which uses \\(p_{(1)}\\) to obtain \\[g_{Tip}(\\mathbf{p}) = 1 - (1 - p_{(1)})^M,\\] the exact version of the commonly used approximation \\(Mp_{(1)}\\) known as the Bonferroni correction. The second kind of pooled \\(p\\)-value makes use of quantile transformations. For a continuous random variable \\(X\\) with cumulative distribution function (CDF) \\(F\\) and quantile function \\(F^{-1}\\), \\(F^{-1}(U) = X\\). If \\(X\\) is known to have a CDF \\(F_M\\) under summation, the quantities \\[g(\\mathbf{p}) = 1 - F_M \\left ( \\sum_{i = 1}^M F^{-1}(1 - p_i) \\right )\\] and \\[g(\\mathbf{p}) = F_M \\left ( \\sum_{i = 1}^M F^{-1}(p_i) \\right )\\] will both behave exactly like a univariate \\(p\\)-value. Obvious choices for \\(X\\) are the normal distribution and gamma distribution because both of these distributions are closed under summation. The majority of proposed pooled \\(p\\)-values follow the first of these quantile transformation methods, including \\(g_{Fis}\\) of Fisher (1932) (using the \\(\\chi^2_2\\) distribution) and \\(g_{Sto}\\) of Stouffer et al. (1949) (using the \\(N(0,1)\\) distribution). Let’s code all of these up: ## the order statistic function gord &lt;- function(p, k) { pk &lt;- sort(p, decreasing = FALSE)[k] # kth order statistic 1 - pbinom(k, length(p), pk) } ## the quantile transformation function gquant &lt;- function(p, finv, fm) { 1 - fm(sum(finv(1 - p))) } ## make particular instances gtip &lt;- function(p) gord(p, 1) gfis &lt;- function(p) gquant(p, finv = function(p) qchisq(p, 2), fm = function(s) pchisq(s, 2 * length(p))) gsto &lt;- function(p) gquant(p, finv = qnorm, fm = function(s) pnorm(s, sd = sqrt(length(p)))) ## store together in a list pools &lt;- list(tippett = gtip, fisher = gfis, stouffer = gsto) We can now compute the pooled \\(p\\)-values of our earlier Mendelian and non-Mendelian cases: (poolMendel &lt;- lapply(pools, function(f) f(pMendel))) ## $tippett ## [1] 0 ## ## $fisher ## [1] 0 ## ## $stouffer ## [1] 0 (poolNonMend &lt;- lapply(pools, function(f) f(pnonMendel))) ## $tippett ## [1] 0.2025844 ## ## $fisher ## [1] 0.01703636 ## ## $stouffer ## [1] 0.005196858 All methods agree for the first set of \\(p\\)-values (not too surprising given that the first \\(p\\)-value is less than \\(1 \\times 10^{-27}\\)), but the non-Mendelian case leads to some interesting differences. Let’s look at the two sets of \\(p\\)-values again: rbind(Mendel = round(pMendel, 2), `Non-Mendel` = round(pnonMendel, 2)) ## D1Mit475 D2Mit312 D3Mit60 D4Mit149 D5Mit331 D6Mit86 D7Mit21 Fcer2a ## Mendel 0.00 0.56 0.57 0.72 0.77 0.31 0.51 0.54 ## Non-Mendel 0.85 0.14 0.09 0.19 0.08 0.41 0.70 0.05 ## D9Mit186 D10Mit166 D11Mit1 D12Mit37 D13Mit158 D14Mit179 D15Mit12 ## Mendel 0.67 0.41 0.04 0.29 0.55 0.21 0.40 ## Non-Mendel 0.15 0.15 0.04 0.16 0.42 0.57 0.82 ## D16Mit32 Tfb1m D18Mit66 D19Mit32 DXMit26 ## Mendel 0.14 0.10 0.57 0.77 0.86 ## Non-Mendel 0.52 0.11 0.19 0.09 0.82 When only one marker is responsible for the variation in the trait, a single \\(p\\)-value is highly significant. In the case of many responsible markers the \\(p\\)-values are not so obviously significant, with the smallest one barely less than 0.05. Rather, this case has a greater number of small \\(p\\)-values than we would expect by chance if all are truly uniform and independent, a fact more easily seen by viewing a histogram of the two sets of \\(p\\)-values. hist(pMendel, breaks = seq(0, 1, by = 0.2), ylim = c(0, 12)) hist(pnonMendel, breaks = seq(0, 1, by = 0.2), ylim = c(0, 12)) Though the proof is beyond the scope of this workshop, it turns out the ordering of \\(g_{Tip}\\), \\(g_{Fis}\\), and \\(g_{Sto}\\) for the non-Mendelian case reflects a deeper concept. Broadly, any pooled \\(p\\)-value is sensitive to either a large number of weakly significant \\(p\\)-values or a small number of strongly significant \\(p\\)-values. There is a fundamental trade-off between these two cases for any pooled \\(p\\)-value, which we can visualize crudely using the mean and minimum values of each simulated null sample in pNull. The following plot shows the pooled \\(p\\)-value for each method by the mean of the sample, meant to represent the evidence spread broadly. Point sizes for each mean value are scaled by the minimum \\(p\\)-value, meant to represent concentrated evidence in one test. ## compute medians for each sample pNullMean &lt;- apply(t(pNull), 1, mean) ## and the minimum pNullMin &lt;- apply(t(pNull), 1, min) ## as well as each pooled p-value pNullPools &lt;- apply(t(pNull), 1, function(row) sapply(pools, function(f) f(row))) ## plot the pooled p-value by mean of sample plot(pNullMean, pNullPools[&quot;fisher&quot;, ], pch = 16, ylab = &quot;Pooled p-value&quot;, cex = pNullMin * 4 + 0.3, xlab = &quot;Mean of p-value sample&quot;, col = adjustcolor(&quot;firebrick&quot;, 0.3)) points(pNullMean, pNullPools[&quot;stouffer&quot;, ], pch = 16, cex = pNullMin * 4 + 0.3, col = adjustcolor(&quot;steelblue&quot;, 0.3)) points(pNullMean, pNullPools[&quot;tippett&quot;, ], pch = 16, cex = pNullMin * 4 + 0.3, col = adjustcolor(&quot;seagreen&quot;, 0.3)) legend(x = &quot;topleft&quot;, col = adjustcolor(c(&quot;firebrick&quot;, &quot;steelblue&quot;, &quot;seagreen&quot;), 0.3), legend = c(expression(g[Fis]), expression(g[Sto]), expression(g[Tip])), pch = 16) Note how \\(g_{Sto}\\) is increasing almost monotonically in the mean, with large and small points spread evenly throughout the line for each mean value. \\(g_{Fis}\\), as suggested by its position giving a \\(p\\)-value between \\(g_{Sto}\\) and \\(g_{Tip}\\), follows this line somewhat, but is somewhat sensitive to the minimum. The large points, indicating samples with large minimum values, tend to have a larger \\(p\\)-value than the \\(g_{Sto}\\) line while the small points, samples with small minimum values, tend to have a smaller \\(p\\)-value. \\(g_{Tip}\\) barely follows the line at all, the only relationship we can see in the green points is between the size of the point and its \\(y\\)-value. All of this to say: \\(g_{Sto}\\) is not sensitive to the minimum, it follows the overall evidence spread among the \\(p\\)-values \\(g_{Fis}\\) balances sensitivity to the minimum and overall spread of \\(p\\)-values \\(g_{Tip}\\) is not sensitive to the overall spread of evidence, it only responds to the minimum Of course, these examples all assume independence of the \\(p\\)-values. In the case of dependence, the idea of what “spread” evidence means is a lot less clear. Are multiple small \\(p\\)-values an indication of multiple independent contributing variables, or simply an artifact of correlations between them? 8.3 Adjusting for dependence A comprehensive overview of the methods used to adjust for dependence can be found in the companion paper to the poolr package, Cinar and Viechtbauer (2022). It should come as no surprise that the above functions have been implemented in this package alongside the adjustments. So, instead of using the functions we wrote above, we can write library(poolr) ## the naming is more convenient than ours cat(&quot;Mendelian p-values\\n\\n&quot;) ## Mendelian p-values fisher(pMendel, adjust = &quot;none&quot;) ## combined p-values with: Fisher&#39;s method ## number of p-values combined: 20 ## test statistic: 161.118 ~ chi-square(df = 40) ## adjustment: none ## combined p-value: 1.817574e-16 cat(&quot;-------\\n&quot;) ## ------- stouffer(pMendel, adjust = &quot;none&quot;) ## combined p-values with: Stouffer&#39;s method ## number of p-values combined: 20 ## test statistic: 2.894 ~ N(0,1) ## adjustment: none ## combined p-value: 0.00190034 cat(&quot;-------\\n&quot;) ## ------- tippett(pMendel, adjust = &quot;none&quot;) # more accurate for small p-values ## combined p-values with: Tippett&#39;s method ## number of p-values combined: 20 ## minimum p-value: 0 ## adjustment: none ## combined p-value: 0 We might immediately notice that this is more accurate for the small \\(p\\)-value occurring in the Mendelian example. This is a direct result of the way gquant is defined, we can achieve the same result as poolr by using the lower.tail argument rather than subtracting values from 1. Computational statistics is not quite the same as mathematical statistics! pnorm(sum(qnorm(pMendel, lower.tail = FALSE)), sd = sqrt(20), lower.tail = FALSE) ## [1] 0.00190034 8.3.1 The wrong way 8.3.2 A better way "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
